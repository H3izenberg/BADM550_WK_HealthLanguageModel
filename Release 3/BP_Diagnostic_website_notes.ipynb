{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Only One API"
      ],
      "metadata": {
        "id": "L8ENORlhkq0r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QzaAHt_iaKv",
        "outputId": "4fd91efc-b908-45ac-f044-231cbd68fe86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "\n",
        "API_KEY = \"AIzaSyCLaZBqHGwWwFZHiizbS0GNo52z6Qap8mk\"  # ← 这里是你给的 API Key\n",
        "MODEL = \"models/gemini-1.5-pro\"\n",
        "URL = f\"https://generativelanguage.googleapis.com/v1beta/{MODEL}:generateContent?key={API_KEY}\"\n"
      ],
      "metadata": {
        "id": "_HnfQBsbigaE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diagnose(age, sex, symptoms, history, labs, imaging):\n",
        "    patient_info = f\"\"\"\n",
        "    Age: {age}\n",
        "    Sex: {sex}\n",
        "    Symptoms: {symptoms}\n",
        "    Medical History: {history}\n",
        "    Lab Values: {labs}\n",
        "    Imaging: {imaging}\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are one of the world's top physicians with access to global medical databases such as Mayo Clinic, NEJM, UpToDate, and JAMA.\n",
        "Analyze the following patient case and generate:\n",
        "1. A ranked list of the top 8 most likely diagnoses,\n",
        "2. Each with a probability (in %) and\n",
        "3. A short clinical justification (1–2 lines).\n",
        "\n",
        "Patient Case:\n",
        "{patient_info}\n",
        "\n",
        "Format:\n",
        "1. Diagnosis A - 60% - Reason\n",
        "2. Diagnosis B - 20% - Reason\n",
        "...\n",
        "    \"\"\"\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"parts\": [{\"text\": prompt}]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(URL, headers=headers, data=json.dumps(data))\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            full_text = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "            matches = re.findall(r\"\\d+\\.\\s+.*?(?=\\n\\d+\\.|\\Z)\", full_text.strip(), re.DOTALL)\n",
        "            results = matches[:8]\n",
        "            results += [\"\"] * (8 - len(results))  # Pad to 8 if fewer results\n",
        "            return results\n",
        "        else:\n",
        "            return [f\"❌ API Error {response.status_code}\"] * 8\n",
        "\n",
        "    except Exception as e:\n",
        "        return [f\"❌ System Error: {str(e)}\"] * 8\n"
      ],
      "metadata": {
        "id": "Vl7a5073ih8w"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=diagnose,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Age\"),\n",
        "        gr.Dropdown(choices=[\"Male\", \"Female\", \"Other\"], label=\"Sex\"),\n",
        "        gr.Textbox(label=\"Symptoms\"),\n",
        "        gr.Textbox(label=\"Medical History\"),\n",
        "        gr.Textbox(label=\"Lab Values\"),\n",
        "        gr.Textbox(label=\"Imaging Results\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=f\"Diagnosis #{i+1}\") for i in range(8)\n",
        "    ],\n",
        "    title=\"🧠 AI Medical Diagnosis (Gemini-powered)\",\n",
        "    description=\"Fill in the patient info to get the Top 8 likely diagnoses with reasons.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "kuVwcRAviiU-",
        "outputId": "29cc748e-a2f3-4d50-ab4d-b69c4182aa02"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cc2e8b863bb682faf9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cc2e8b863bb682faf9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple API integration"
      ],
      "metadata": {
        "id": "R7CV0zFPkuWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Gemini\n",
        "GEMINI_KEY = \"AIzaSyCLaZBqHGwWwFZHiizbS0GNo52z6Qap8mk\"\n",
        "GEMINI_MODEL = \"models/gemini-1.5-pro\"\n",
        "GEMINI_URL = f\"https://generativelanguage.googleapis.com/v1beta/{GEMINI_MODEL}:generateContent?key={GEMINI_KEY}\"\n",
        "\n",
        "# DeepSeek via OpenRouter\n",
        "OPENROUTER_KEY = \"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\"\n",
        "DEEPSEEK_MODEL = \"deepseek-chat\"\n",
        "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n"
      ],
      "metadata": {
        "id": "BhXkBdSwkuFY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_gemini(prompt):\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(GEMINI_URL, headers=headers, data=json.dumps(data))\n",
        "        text = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        results = re.findall(r\"\\d+\\.\\s+.*?(?=\\n\\d+\\.|\\Z)\", text.strip(), re.DOTALL)\n",
        "        return results[:8]\n",
        "    except Exception as e:\n",
        "        return [f\"[Gemini Error] {str(e)}\"]\n"
      ],
      "metadata": {
        "id": "YAKFwe5eljA7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_deepseek(prompt):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": DEEPSEEK_MODEL,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(OPENROUTER_URL, headers=headers, data=json.dumps(data))\n",
        "        text = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "        results = re.findall(r\"\\d+\\.\\s+.*?(?=\\n\\d+\\.|\\Z)\", text.strip(), re.DOTALL)\n",
        "        return results[:8]\n",
        "    except Exception as e:\n",
        "        return [f\"[DeepSeek Error] {str(e)}\"]\n"
      ],
      "metadata": {
        "id": "5Q8KUv8ZlknM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_ai_diagnose(age, sex, symptoms, history, labs, imaging):\n",
        "    patient_info = f\"\"\"\n",
        "    Age: {age}\n",
        "    Sex: {sex}\n",
        "    Symptoms: {symptoms}\n",
        "    Medical History: {history}\n",
        "    Lab Values: {labs}\n",
        "    Imaging: {imaging}\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are one of the world's top physicians with access to databases like Mayo Clinic, NEJM, UpToDate, and JAMA.\n",
        "Given the patient case, generate a ranked list of 8 most likely diagnoses with percentage and reason.\n",
        "\n",
        "Format:\n",
        "1. Disease A - 60% - Explanation\n",
        "2. Disease B - 20% - Explanation\n",
        "...\n",
        "Patient Case:\n",
        "{patient_info}\n",
        "\"\"\"\n",
        "\n",
        "    gemini_results = call_gemini(prompt)\n",
        "    deepseek_results = call_deepseek(prompt)\n",
        "\n",
        "    combined = []\n",
        "    for i in range(8):\n",
        "        g = gemini_results[i] if i < len(gemini_results) else \"\"\n",
        "        d = deepseek_results[i] if i < len(deepseek_results) else \"\"\n",
        "        combined.append(f\"Gemini: {g}\\nDeepSeek: {d}\")\n",
        "\n",
        "    return combined\n"
      ],
      "metadata": {
        "id": "tq-tbaK2lqVy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=multi_ai_diagnose,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Age\"),\n",
        "        gr.Dropdown(choices=[\"Male\", \"Female\", \"Other\"], label=\"Sex\"),\n",
        "        gr.Textbox(label=\"Symptoms\"),\n",
        "        gr.Textbox(label=\"Medical History\"),\n",
        "        gr.Textbox(label=\"Lab Values\"),\n",
        "        gr.Textbox(label=\"Imaging Results\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=f\"Combined Diagnosis #{i+1}\") for i in range(8)\n",
        "    ],\n",
        "    title=\"🌐 Multi-AI Diagnosis Assistant (Gemini + DeepSeek)\",\n",
        "    description=\"Top 8 possible diagnoses based on Gemini and DeepSeek outputs with explanations.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "E53NmM6KlsgJ",
        "outputId": "eb245446-dc1f-47f7-f085-1edc000171cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0bf92d483de86115f7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0bf92d483de86115f7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}