{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting JSON of Correct/Final Diagnosis from Case Records"
      ],
      "metadata": {
        "id": "AbzQ0Gqr2p4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary library ---\n",
        "\n",
        "!pip install PyMuPDF -q # -q for quiet installation\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import fitz  # PyMuPDF library\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Functions (find_files, extract_text_from_pdf, extract_diagnosis_from_text) ---\n",
        "\n",
        "def find_files(directory, extension=\".pdf\"):\n",
        "    \"\"\"Finds all files with a given extension in a directory, sorted alphabetically.\"\"\"\n",
        "    files = []\n",
        "    try:\n",
        "        if not os.path.isdir(directory):\n",
        "             print(f\"Error: Directory not found or not accessible: {directory}\")\n",
        "             return None\n",
        "        # Sort files alphabetically for consistent processing order\n",
        "        for filename in sorted(os.listdir(directory)):\n",
        "            if filename.lower().endswith(extension.lower()):\n",
        "                files.append(os.path.join(directory, filename))\n",
        "        if not files:\n",
        "             print(f\"Warning: No files with extension '{extension}' found in directory '{directory}'.\")\n",
        "        return files\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred listing directory contents: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_from_pdf(filepath):\n",
        "    \"\"\"Extracts all text content from a PDF file.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(filepath)\n",
        "        full_text = \"\"\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            full_text += page.get_text(\"text\") # Extract text from the page\n",
        "        doc.close()\n",
        "        # Basic text cleaning\n",
        "        full_text = re.sub(r'\\n\\s*\\n', '\\n\\n', full_text)\n",
        "        full_text = re.sub(r' +', ' ', full_text)\n",
        "        return full_text\n",
        "    except fitz.fitz.FileNotFoundError:\n",
        "        print(f\"Error: File not found by PyMuPDF: {filepath}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF {os.path.basename(filepath)}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_diagnosis_from_text(full_text, marker=\"FINAL DIAGNOSIS\", filename=\"Unknown\"):\n",
        "    \"\"\"\n",
        "    Extracts text following the 'FINAL DIAGNOSIS' marker, tailored for NEJM format.\n",
        "    \"\"\"\n",
        "    if not full_text:\n",
        "        return None\n",
        "\n",
        "    marker_pattern = re.compile(r\"^\\s*\" + re.escape(marker) + r\"\\s*$\", re.IGNORECASE | re.MULTILINE)\n",
        "    match = marker_pattern.search(full_text)\n",
        "\n",
        "    if not match:\n",
        "        return None # Marker not found\n",
        "\n",
        "    start_index = match.end()\n",
        "    text_after_marker = full_text[start_index:]\n",
        "    lines = text_after_marker.splitlines()\n",
        "    extracted_lines = []\n",
        "    found_diagnosis_line = False\n",
        "\n",
        "    for line in lines:\n",
        "        stripped_line = line.strip()\n",
        "        if stripped_line:\n",
        "            stop_keywords = [\"Disclosure forms provided\", \"References\", \"This case was presented\"]\n",
        "            if any(keyword.lower() in stripped_line.lower() for keyword in stop_keywords) and found_diagnosis_line:\n",
        "                 break\n",
        "            extracted_lines.append(stripped_line)\n",
        "            found_diagnosis_line = True\n",
        "        elif found_diagnosis_line:\n",
        "            break\n",
        "        if len(extracted_lines) > 5:\n",
        "            break\n",
        "\n",
        "    if not extracted_lines:\n",
        "         return \"\" # Marker found but no text followed\n",
        "\n",
        "    diagnosis_text = \" \".join(extracted_lines)\n",
        "    diagnosis_text = re.sub(r'\\s+', ' ', diagnosis_text).strip()\n",
        "    return diagnosis_text\n",
        "\n",
        "# --- Configuration ---\n",
        "DIAGNOSIS_MARKER = \"FINAL DIAGNOSIS\"\n",
        "FILE_EXTENSION = \".pdf\"\n",
        "DEFAULT_OUTPUT_FILENAME = \"ground_truth_diagnoses.json\" # Name of the output file\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Google Drive Mounting ---\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        DRIVE_ROOT = \"/content/drive/MyDrive/\"\n",
        "        print(f\"Google Drive mounted. Your 'My Drive' is at: {DRIVE_ROOT}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Get User Input for Folder Path using Colab File Browser ---\n",
        "    print(\"\\n--- Please specify the folder containing your PDF files ---\")\n",
        "    print(\"1. Look at the 'Files' panel on the left side of Colab.\")\n",
        "    print(\"2. Navigate through 'drive' -> 'MyDrive' to find your folder.\")\n",
        "    print(\"3. Right-click on the correct folder.\")\n",
        "    print(\"4. Select 'Copy path' from the menu.\")\n",
        "    print(\"5. Paste the copied path into the prompt below and press Enter.\")\n",
        "    print(\"-\" * 60)\n",
        "    input_directory_path = input(\"Paste the full path to the folder here: \")\n",
        "\n",
        "    # --- Clean and Validate the Input Path ---\n",
        "    INPUT_DIRECTORY = input_directory_path.strip()\n",
        "    if not INPUT_DIRECTORY:\n",
        "        print(\"Error: No path provided. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Define Output Path (saving to the root of My Drive)\n",
        "    OUTPUT_JSON_FILE = os.path.join(DRIVE_ROOT, DEFAULT_OUTPUT_FILENAME)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Attempting to read files from: {INPUT_DIRECTORY}\")\n",
        "    print(f\"Output JSON will be saved to: {OUTPUT_JSON_FILE}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # --- Check if Input Directory Exists ---\n",
        "    if not os.path.isdir(INPUT_DIRECTORY):\n",
        "        print(f\"Error: The path you provided is not a valid directory: '{INPUT_DIRECTORY}'\")\n",
        "    else:\n",
        "        # --- Find and Process Files ---\n",
        "        case_files = find_files(INPUT_DIRECTORY, extension=FILE_EXTENSION)\n",
        "        all_diagnoses = []\n",
        "        files_processed_count = 0\n",
        "        marker_not_found_count = 0\n",
        "        extraction_error_count = 0\n",
        "        total_files_attempted = 0\n",
        "\n",
        "        if case_files: # Check if find_files returned a list\n",
        "            total_files_attempted = len(case_files)\n",
        "            print(f\"Found {total_files_attempted} PDF files. Processing...\")\n",
        "            for idx, file_path in enumerate(case_files): # Use enumerate for progress count\n",
        "                filename = os.path.basename(file_path)\n",
        "                print(f\"Processing ({idx + 1}/{total_files_attempted}): {filename}...\")\n",
        "\n",
        "                # Step 1: Extract text from PDF\n",
        "                full_pdf_text = extract_text_from_pdf(file_path)\n",
        "\n",
        "                if full_pdf_text:\n",
        "                    # Step 2: Extract diagnosis using the refined logic\n",
        "                    diagnosis = extract_diagnosis_from_text(full_pdf_text, DIAGNOSIS_MARKER, filename)\n",
        "\n",
        "                    if diagnosis is not None: # None means marker wasn't found\n",
        "                        # --- Generate case_id from filename (Exact base name) ---\n",
        "                        case_id, _ = os.path.splitext(filename) # Gets filename without extension\n",
        "                        # --- End case_id generation ---\n",
        "\n",
        "                        all_diagnoses.append({\n",
        "                            \"case_id\": case_id,\n",
        "                            \"correct_diagnosis\": diagnosis # diagnosis can be \"\"\n",
        "                        })\n",
        "                        files_processed_count += 1\n",
        "                        if diagnosis == \"\":\n",
        "                             print(f\"--> Warning: Marker found but no diagnosis text extracted for {filename}.\")\n",
        "                    else:\n",
        "                        # Marker not found\n",
        "                        print(f\"--> Warning: Marker '{DIAGNOSIS_MARKER}' not found in {filename}.\")\n",
        "                        marker_not_found_count += 1\n",
        "                else:\n",
        "                     # Error message printed in extract_text_from_pdf\n",
        "                     print(f\"--> Error: PDF text extraction failed for {filename}.\")\n",
        "                     extraction_error_count += 1\n",
        "\n",
        "            # --- Save Results to JSON in Google Drive ---\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"Processing Summary:\")\n",
        "            print(f\" - Files successfully processed (marker found): {files_processed_count}\")\n",
        "            print(f\" - Files where marker was not found: {marker_not_found_count}\")\n",
        "            print(f\" - Files with PDF extraction errors: {extraction_error_count}\")\n",
        "            print(f\" - Total files attempted: {total_files_attempted}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            if all_diagnoses:\n",
        "                try:\n",
        "                    # Save in the order files were processed (which is sorted alphabetically by filename)\n",
        "                    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f_out:\n",
        "                        json.dump(all_diagnoses, f_out, indent=2, ensure_ascii=False)\n",
        "                    print(f\"Successfully generated JSON for {len(all_diagnoses)} cases.\")\n",
        "                    print(f\"Results saved to: {OUTPUT_JSON_FILE}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nError writing JSON output file: {e}\")\n",
        "            else:\n",
        "                print(\"\\nNo diagnoses were successfully extracted to save.\")\n",
        "        else:\n",
        "             # Message handled by find_files or the initial directory check\n",
        "             print(\"Processing complete. No PDF files found in the specified directory.\")\n",
        "\n",
        "# Reminder for the user\n",
        "print(\"\\nScript finished. IMPORTANT: Please review the generated JSON file carefully.\")\n",
        "print(f\"Verify the extracted diagnoses and case_ids in '{OUTPUT_JSON_FILE}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O3ZNhQRzoE0",
        "outputId": "23c08693-0974-4280-b26a-2c6cc4206806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted. Your 'My Drive' is at: /content/drive/MyDrive/\n",
            "\n",
            "--- Please specify the folder containing your PDF files ---\n",
            "1. Look at the 'Files' panel on the left side of Colab.\n",
            "2. Navigate through 'drive' -> 'MyDrive' to find your folder.\n",
            "3. Right-click on the correct folder.\n",
            "4. Select 'Copy path' from the menu.\n",
            "5. Paste the copied path into the prompt below and press Enter.\n",
            "------------------------------------------------------------\n",
            "Paste the full path to the folder here: /content/drive/MyDrive/NEJM Case Records\n",
            "------------------------------\n",
            "Attempting to read files from: /content/drive/MyDrive/NEJM Case Records\n",
            "Output JSON will be saved to: /content/drive/MyDrive/ground_truth_diagnoses.json\n",
            "------------------------------\n",
            "Found 25 PDF files. Processing...\n",
            "Processing (1/25): NEJMcpc2100279.pdf...\n",
            "Processing (2/25): NEJMcpc2300900.pdf...\n",
            "Processing (3/25): NEJMcpc2309383.pdf...\n",
            "Processing (4/25): NEJMcpc2309500.pdf...\n",
            "Processing (5/25): NEJMcpc2309726.pdf...\n",
            "Processing (6/25): NEJMcpc2312734.pdf...\n",
            "Processing (7/25): NEJMcpc2312735.pdf...\n",
            "Processing (8/25): NEJMcpc2402483.pdf...\n",
            "Processing (9/25): NEJMcpc2402485.pdf...\n",
            "Processing (10/25): NEJMcpc2402486.pdf...\n",
            "Processing (11/25): NEJMcpc2402487.pdf...\n",
            "Processing (12/25): NEJMcpc2402488.pdf...\n",
            "Processing (13/25): NEJMcpc2402489.pdf...\n",
            "Processing (14/25): NEJMcpc2402490.pdf...\n",
            "Processing (15/25): NEJMcpc2402491.pdf...\n",
            "Processing (16/25): NEJMcpc2402492.pdf...\n",
            "Processing (17/25): NEJMcpc2402493.pdf...\n",
            "Processing (18/25): NEJMcpc2402496.pdf...\n",
            "Processing (19/25): NEJMcpc2402498.pdf...\n",
            "Processing (20/25): NEJMcpc2402499.pdf...\n",
            "Processing (21/25): NEJMcpc2402500.pdf...\n",
            "Processing (22/25): NEJMcpc2402504.pdf...\n",
            "Processing (23/25): NEJMcpc2402505.pdf...\n",
            "Processing (24/25): NEJMcpc2412511.pdf...\n",
            "Processing (25/25): NEJMcpc2412513.pdf...\n",
            "------------------------------\n",
            "Processing Summary:\n",
            " - Files successfully processed (marker found): 25\n",
            " - Files where marker was not found: 0\n",
            " - Files with PDF extraction errors: 0\n",
            " - Total files attempted: 25\n",
            "------------------------------\n",
            "Successfully generated JSON for 25 cases.\n",
            "Results saved to: /content/drive/MyDrive/ground_truth_diagnoses.json\n",
            "\n",
            "Script finished. IMPORTANT: Please review the generated JSON file carefully.\n",
            "Verify the extracted diagnoses and case_ids in '/content/drive/MyDrive/ground_truth_diagnoses.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collating All Gemini 2.5 Pro Differential Diagnoses (Full Case Record)"
      ],
      "metadata": {
        "id": "qi_2DQ6n23os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Ensure necessary libraries are available ---\n",
        "\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "import glob # Useful for finding files matching a pattern\n",
        "\n",
        "# --- Function to find JSON files ---\n",
        "def find_json_files(directory):\n",
        "    \"\"\"Finds all .json files in a given directory, sorted alphabetically.\"\"\"\n",
        "    files = []\n",
        "    try:\n",
        "        if not os.path.isdir(directory):\n",
        "             print(f\"Error: Directory not found or not accessible: {directory}\")\n",
        "             return None\n",
        "        json_pattern = os.path.join(directory, '*.json')\n",
        "        files = sorted(glob.glob(json_pattern))\n",
        "        if not files:\n",
        "             print(f\"Warning: No .json files found in directory '{directory}'.\")\n",
        "        return files\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred listing or searching directory contents: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Function to parse a single prediction JSON file ---\n",
        "def parse_prediction_file(filepath):\n",
        "    \"\"\"\n",
        "    Parses a single Gemini prediction JSON file and extracts required fields,\n",
        "    assigning rank based on order.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the individual JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted 'case_id' and\n",
        "              'differential_diagnosis' list (with only 'diagnosis' and\n",
        "              'rank'), or None if parsing fails or required\n",
        "              keys are missing.\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(filepath)\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # --- Extract required top-level keys ---\n",
        "        case_id = data.get('case_id')\n",
        "        diff_diag_list = data.get('differential_diagnosis')\n",
        "\n",
        "        if case_id is None:\n",
        "            print(f\"Warning: Missing 'case_id' key in file: {filename}. Skipping.\")\n",
        "            return None\n",
        "        if diff_diag_list is None:\n",
        "            print(f\"Warning: Missing 'differential_diagnosis' key in file: {filename} (Case ID: {case_id}). Skipping.\")\n",
        "            return None\n",
        "        if not isinstance(diff_diag_list, list):\n",
        "             print(f\"Warning: 'differential_diagnosis' is not a list in file: {filename} (Case ID: {case_id}). Skipping.\")\n",
        "             return None\n",
        "\n",
        "        # --- Process the differential diagnosis list ---\n",
        "        processed_diff_diag = []\n",
        "        # Use enumerate to get index (for rank) and item\n",
        "        for index, item in enumerate(diff_diag_list):\n",
        "            if not isinstance(item, dict):\n",
        "                print(f\"Warning: Item in 'differential_diagnosis' is not a dictionary in file: {filename} (Case ID: {case_id}). Skipping item.\")\n",
        "                continue\n",
        "\n",
        "            diagnosis = item.get('diagnosis')\n",
        "            # We no longer need confidence_level = item.get('confidence_level')\n",
        "\n",
        "            if diagnosis is None:\n",
        "                print(f\"Warning: Missing 'diagnosis' key within an item in 'differential_diagnosis' in file: {filename} (Case ID: {case_id}). Skipping item.\")\n",
        "                continue\n",
        "\n",
        "            # Calculate rank (1-based index)\n",
        "            rank = index + 1\n",
        "\n",
        "            processed_diff_diag.append({\n",
        "                \"diagnosis\": diagnosis,\n",
        "                \"rank\": rank  # Use 'rank' key with the calculated rank\n",
        "            })\n",
        "\n",
        "        # Return the structured data for this case\n",
        "        return {\n",
        "            \"case_id\": case_id,\n",
        "            \"differential_diagnosis\": processed_diff_diag\n",
        "        }\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Invalid JSON structure in file: {filename}. Skipping.\")\n",
        "        return None\n",
        "    except FileNotFoundError:\n",
        "         print(f\"Error: File not found during processing: {filepath}. Skipping.\")\n",
        "         return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred processing file {filename}: {e}. Skipping.\")\n",
        "        return None\n",
        "\n",
        "# --- Configuration ---\n",
        "DEFAULT_OUTPUT_FILENAME = \"gemini_2.5_pro_predictions.json\" # Updated filename\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Google Drive Mounting ---\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        DRIVE_ROOT = \"/content/drive/MyDrive/\"\n",
        "        print(f\"Google Drive mounted. Your 'My Drive' is at: {DRIVE_ROOT}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Get User Input for Folder Path ---\n",
        "    print(\"\\n--- Please specify the folder containing the 25 individual Gemini prediction JSON files ---\")\n",
        "    print(\"1. Use the 'Files' panel on the left to navigate to the folder.\")\n",
        "    print(\"2. Right-click on the folder.\")\n",
        "    print(\"3. Select 'Copy path'.\")\n",
        "    print(\"4. Paste the copied path below and press Enter.\")\n",
        "    print(\"-\" * 60)\n",
        "    input_directory_path = input(\"Paste the full path to the folder here: \")\n",
        "\n",
        "    # --- Clean and Validate Input Path ---\n",
        "    INPUT_DIRECTORY = input_directory_path.strip()\n",
        "    if not INPUT_DIRECTORY:\n",
        "        print(\"Error: No path provided. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Define Output Path (saving to the root of My Drive)\n",
        "    OUTPUT_JSON_FILE = os.path.join(DRIVE_ROOT, DEFAULT_OUTPUT_FILENAME)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Reading individual JSON files from: {INPUT_DIRECTORY}\")\n",
        "    print(f\"Collated output with ranks will be saved to: {OUTPUT_JSON_FILE}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # --- Check if Input Directory Exists ---\n",
        "    if not os.path.isdir(INPUT_DIRECTORY):\n",
        "        print(f\"Error: The path provided is not a valid directory: '{INPUT_DIRECTORY}'\")\n",
        "    else:\n",
        "        # --- Find and Process JSON Files ---\n",
        "        prediction_files = find_json_files(INPUT_DIRECTORY)\n",
        "        collated_predictions = []\n",
        "        files_processed_count = 0\n",
        "        files_skipped_count = 0\n",
        "        total_files_found = 0\n",
        "\n",
        "        if prediction_files: # Check if find_json_files returned a list\n",
        "            total_files_found = len(prediction_files)\n",
        "            print(f\"Found {total_files_found} JSON files. Processing...\")\n",
        "\n",
        "            for idx, file_path in enumerate(prediction_files):\n",
        "                filename = os.path.basename(file_path)\n",
        "                print(f\"Processing ({idx + 1}/{total_files_found}): {filename}...\")\n",
        "\n",
        "                # Parse the individual file\n",
        "                parsed_data = parse_prediction_file(file_path)\n",
        "\n",
        "                if parsed_data:\n",
        "                    collated_predictions.append(parsed_data)\n",
        "                    files_processed_count += 1\n",
        "                else:\n",
        "                    # Error/warning message already printed by parse_prediction_file\n",
        "                    files_skipped_count += 1\n",
        "\n",
        "            # --- Save Collated Results ---\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"Processing Summary:\")\n",
        "            print(f\" - Files successfully parsed and included: {files_processed_count}\")\n",
        "            print(f\" - Files skipped due to errors or missing keys: {files_skipped_count}\")\n",
        "            print(f\" - Total JSON files found: {total_files_found}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            if collated_predictions:\n",
        "                try:\n",
        "                    # Optional: Sort the final list by case_id if needed\n",
        "                    # collated_predictions.sort(key=lambda x: x.get('case_id', ''))\n",
        "\n",
        "                    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f_out:\n",
        "                        json.dump(collated_predictions, f_out, indent=2, ensure_ascii=False)\n",
        "                    print(f\"Successfully generated collated predictions file with ranks.\")\n",
        "                    print(f\"Results saved to: {OUTPUT_JSON_FILE}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nError writing final JSON output file: {e}\")\n",
        "            else:\n",
        "                print(\"\\nNo data was successfully parsed from any file. Output file not created.\")\n",
        "        else:\n",
        "             # Message handled by find_json_files or the initial directory check\n",
        "             print(\"Processing complete. No JSON files found in the specified directory.\")\n",
        "\n",
        "# Reminder\n",
        "print(\"\\nScript finished.\")\n",
        "if files_processed_count > 0:\n",
        "    print(f\"Please check the generated file: {OUTPUT_JSON_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o553g7pVTosd",
        "outputId": "6154c826-cf9e-4abb-c8e4-7d64eaf181c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted. Your 'My Drive' is at: /content/drive/MyDrive/\n",
            "\n",
            "--- Please specify the folder containing the 25 individual Gemini prediction JSON files ---\n",
            "1. Use the 'Files' panel on the left to navigate to the folder.\n",
            "2. Right-click on the folder.\n",
            "3. Select 'Copy path'.\n",
            "4. Paste the copied path below and press Enter.\n",
            "------------------------------------------------------------\n",
            "Paste the full path to the folder here: /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/Gemini 2.5 Pro Full Case Records\n",
            "------------------------------\n",
            "Reading individual JSON files from: /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/Gemini 2.5 Pro Full Case Records\n",
            "Collated output with ranks will be saved to: /content/drive/MyDrive/gemini_2.5_pro_predictions_ranked.json\n",
            "------------------------------\n",
            "Found 25 JSON files. Processing...\n",
            "Processing (1/25): NEJMcpc2100279.json...\n",
            "Processing (2/25): NEJMcpc2300900.json...\n",
            "Processing (3/25): NEJMcpc2309383.json...\n",
            "Processing (4/25): NEJMcpc2309500.json...\n",
            "Processing (5/25): NEJMcpc2309726.json...\n",
            "Processing (6/25): NEJMcpc2312734.json...\n",
            "Processing (7/25): NEJMcpc2312735.json...\n",
            "Processing (8/25): NEJMcpc2402483.json...\n",
            "Processing (9/25): NEJMcpc2402485.json...\n",
            "Processing (10/25): NEJMcpc2402486.json...\n",
            "Processing (11/25): NEJMcpc2402487.json...\n",
            "Processing (12/25): NEJMcpc2402488.json...\n",
            "Processing (13/25): NEJMcpc2402489.json...\n",
            "Processing (14/25): NEJMcpc2402490.json...\n",
            "Processing (15/25): NEJMcpc2402491.json...\n",
            "Processing (16/25): NEJMcpc2402492.json...\n",
            "Processing (17/25): NEJMcpc2402493.json...\n",
            "Processing (18/25): NEJMcpc2402496.json...\n",
            "Processing (19/25): NEJMcpc2402498.json...\n",
            "Processing (20/25): NEJMcpc2402499.json...\n",
            "Processing (21/25): NEJMcpc2402500.json...\n",
            "Processing (22/25): NEJMcpc2402504.json...\n",
            "Processing (23/25): NEJMcpc2402505.json...\n",
            "Processing (24/25): NEJMcpc2412511.json...\n",
            "Processing (25/25): NEJMcpc2412513.json...\n",
            "------------------------------\n",
            "Processing Summary:\n",
            " - Files successfully parsed and included: 25\n",
            " - Files skipped due to errors or missing keys: 0\n",
            " - Total JSON files found: 25\n",
            "------------------------------\n",
            "Successfully generated collated predictions file with ranks.\n",
            "Results saved to: /content/drive/MyDrive/gemini_2.5_pro_predictions_ranked.json\n",
            "\n",
            "Script finished.\n",
            "Please check the generated file: /content/drive/MyDrive/gemini_2.5_pro_predictions_ranked.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collating All Grok 3 Differential Diagnoses (Full Case Record)"
      ],
      "metadata": {
        "id": "U4MDTMDGy4Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Ensure necessary libraries are available ---\n",
        "\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "import glob # Useful for finding files matching a pattern\n",
        "\n",
        "# --- Function to find JSON files ---\n",
        "def find_json_files(directory):\n",
        "    \"\"\"Finds all .json files in a given directory, sorted alphabetically.\"\"\"\n",
        "    files = []\n",
        "    try:\n",
        "        if not os.path.isdir(directory):\n",
        "             print(f\"Error: Directory not found or not accessible: {directory}\")\n",
        "             return None\n",
        "        json_pattern = os.path.join(directory, '*.json')\n",
        "        files = sorted(glob.glob(json_pattern))\n",
        "        if not files:\n",
        "             print(f\"Warning: No .json files found in directory '{directory}'.\")\n",
        "        return files\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred listing or searching directory contents: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Function to parse a single prediction JSON file ---\n",
        "def parse_prediction_file(filepath):\n",
        "    \"\"\"\n",
        "    Parses a single Grok prediction JSON file and extracts required fields,\n",
        "    assigning rank based on order.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the individual JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted 'case_id' and\n",
        "              'differential_diagnosis' list (with only 'diagnosis' and\n",
        "              'rank'), or None if parsing fails or required\n",
        "              keys are missing.\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(filepath)\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # --- Extract required top-level keys ---\n",
        "        case_id = data.get('case_id')\n",
        "        diff_diag_list = data.get('differential_diagnosis')\n",
        "\n",
        "        if case_id is None:\n",
        "            print(f\"Warning: Missing 'case_id' key in file: {filename}. Skipping.\")\n",
        "            return None\n",
        "        if diff_diag_list is None:\n",
        "            print(f\"Warning: Missing 'differential_diagnosis' key in file: {filename} (Case ID: {case_id}). Skipping.\")\n",
        "            return None\n",
        "        if not isinstance(diff_diag_list, list):\n",
        "             print(f\"Warning: 'differential_diagnosis' is not a list in file: {filename} (Case ID: {case_id}). Skipping.\")\n",
        "             return None\n",
        "\n",
        "        # --- Process the differential diagnosis list ---\n",
        "        processed_diff_diag = []\n",
        "        # Use enumerate to get index (for rank) and item\n",
        "        for index, item in enumerate(diff_diag_list):\n",
        "            if not isinstance(item, dict):\n",
        "                print(f\"Warning: Item in 'differential_diagnosis' is not a dictionary in file: {filename} (Case ID: {case_id}). Skipping item.\")\n",
        "                continue\n",
        "\n",
        "            diagnosis = item.get('diagnosis')\n",
        "            # We no longer need confidence_level = item.get('confidence_level')\n",
        "\n",
        "            if diagnosis is None:\n",
        "                print(f\"Warning: Missing 'diagnosis' key within an item in 'differential_diagnosis' in file: {filename} (Case ID: {case_id}). Skipping item.\")\n",
        "                continue\n",
        "\n",
        "            # Calculate rank (1-based index)\n",
        "            rank = index + 1\n",
        "\n",
        "            processed_diff_diag.append({\n",
        "                \"diagnosis\": diagnosis,\n",
        "                \"rank\": rank  # Use 'rank' key with the calculated rank\n",
        "            })\n",
        "\n",
        "        # Return the structured data for this case\n",
        "        return {\n",
        "            \"case_id\": case_id,\n",
        "            \"differential_diagnosis\": processed_diff_diag\n",
        "        }\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Invalid JSON structure in file: {filename}. Skipping.\")\n",
        "        return None\n",
        "    except FileNotFoundError:\n",
        "         print(f\"Error: File not found during processing: {filepath}. Skipping.\")\n",
        "         return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred processing file {filename}: {e}. Skipping.\")\n",
        "        return None\n",
        "\n",
        "# --- Configuration ---\n",
        "DEFAULT_OUTPUT_FILENAME = \"grok_3_predictions.json\" # Updated filename\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Google Drive Mounting ---\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        DRIVE_ROOT = \"/content/drive/MyDrive/\"\n",
        "        print(f\"Google Drive mounted. Your 'My Drive' is at: {DRIVE_ROOT}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Get User Input for Folder Path ---\n",
        "    print(\"\\n--- Please specify the folder containing the 25 individual Grok prediction JSON files ---\")\n",
        "    print(\"1. Use the 'Files' panel on the left to navigate to the folder.\")\n",
        "    print(\"2. Right-click on the folder.\")\n",
        "    print(\"3. Select 'Copy path'.\")\n",
        "    print(\"4. Paste the copied path below and press Enter.\")\n",
        "    print(\"-\" * 60)\n",
        "    input_directory_path = input(\"Paste the full path to the folder here: \")\n",
        "\n",
        "    # --- Clean and Validate Input Path ---\n",
        "    INPUT_DIRECTORY = input_directory_path.strip()\n",
        "    if not INPUT_DIRECTORY:\n",
        "        print(\"Error: No path provided. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Define Output Path (saving to the root of My Drive)\n",
        "    OUTPUT_JSON_FILE = os.path.join(DRIVE_ROOT, DEFAULT_OUTPUT_FILENAME)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Reading individual JSON files from: {INPUT_DIRECTORY}\")\n",
        "    print(f\"Collated output with ranks will be saved to: {OUTPUT_JSON_FILE}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # --- Check if Input Directory Exists ---\n",
        "    if not os.path.isdir(INPUT_DIRECTORY):\n",
        "        print(f\"Error: The path provided is not a valid directory: '{INPUT_DIRECTORY}'\")\n",
        "    else:\n",
        "        # --- Find and Process JSON Files ---\n",
        "        prediction_files = find_json_files(INPUT_DIRECTORY)\n",
        "        collated_predictions = []\n",
        "        files_processed_count = 0\n",
        "        files_skipped_count = 0\n",
        "        total_files_found = 0\n",
        "\n",
        "        if prediction_files: # Check if find_json_files returned a list\n",
        "            total_files_found = len(prediction_files)\n",
        "            print(f\"Found {total_files_found} JSON files. Processing...\")\n",
        "\n",
        "            for idx, file_path in enumerate(prediction_files):\n",
        "                filename = os.path.basename(file_path)\n",
        "                print(f\"Processing ({idx + 1}/{total_files_found}): {filename}...\")\n",
        "\n",
        "                # Parse the individual file\n",
        "                parsed_data = parse_prediction_file(file_path)\n",
        "\n",
        "                if parsed_data:\n",
        "                    collated_predictions.append(parsed_data)\n",
        "                    files_processed_count += 1\n",
        "                else:\n",
        "                    # Error/warning message already printed by parse_prediction_file\n",
        "                    files_skipped_count += 1\n",
        "\n",
        "            # --- Save Collated Results ---\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"Processing Summary:\")\n",
        "            print(f\" - Files successfully parsed and included: {files_processed_count}\")\n",
        "            print(f\" - Files skipped due to errors or missing keys: {files_skipped_count}\")\n",
        "            print(f\" - Total JSON files found: {total_files_found}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            if collated_predictions:\n",
        "                try:\n",
        "                    # Optional: Sort the final list by case_id if needed\n",
        "                    # collated_predictions.sort(key=lambda x: x.get('case_id', ''))\n",
        "\n",
        "                    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f_out:\n",
        "                        json.dump(collated_predictions, f_out, indent=2, ensure_ascii=False)\n",
        "                    print(f\"Successfully generated collated predictions file with ranks.\")\n",
        "                    print(f\"Results saved to: {OUTPUT_JSON_FILE}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nError writing final JSON output file: {e}\")\n",
        "            else:\n",
        "                print(\"\\nNo data was successfully parsed from any file. Output file not created.\")\n",
        "        else:\n",
        "             # Message handled by find_json_files or the initial directory check\n",
        "             print(\"Processing complete. No JSON files found in the specified directory.\")\n",
        "\n",
        "# Reminder\n",
        "print(\"\\nScript finished.\")\n",
        "if files_processed_count > 0:\n",
        "    print(f\"Please check the generated file: {OUTPUT_JSON_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xMjF0vzx50_",
        "outputId": "ba7517f6-fc55-4756-c8bf-9f20dcbf6d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted. Your 'My Drive' is at: /content/drive/MyDrive/\n",
            "\n",
            "--- Please specify the folder containing the 25 individual Grok prediction JSON files ---\n",
            "1. Use the 'Files' panel on the left to navigate to the folder.\n",
            "2. Right-click on the folder.\n",
            "3. Select 'Copy path'.\n",
            "4. Paste the copied path below and press Enter.\n",
            "------------------------------------------------------------\n",
            "Paste the full path to the folder here: /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/Grok 3 Full Case Records\n",
            "------------------------------\n",
            "Reading individual JSON files from: /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/Grok 3 Full Case Records\n",
            "Collated output with ranks will be saved to: /content/drive/MyDrive/grok_3_predictions.json\n",
            "------------------------------\n",
            "Found 25 JSON files. Processing...\n",
            "Processing (1/25): NEJMcpc2100279.json...\n",
            "Processing (2/25): NEJMcpc2300900.json...\n",
            "Processing (3/25): NEJMcpc2309383.json...\n",
            "Processing (4/25): NEJMcpc2309500.json...\n",
            "Processing (5/25): NEJMcpc2309726.json...\n",
            "Processing (6/25): NEJMcpc2312734.json...\n",
            "Processing (7/25): NEJMcpc2312735.json...\n",
            "Processing (8/25): NEJMcpc2402483.json...\n",
            "Processing (9/25): NEJMcpc2402485.json...\n",
            "Processing (10/25): NEJMcpc2402486.json...\n",
            "Processing (11/25): NEJMcpc2402487.json...\n",
            "Processing (12/25): NEJMcpc2402488.json...\n",
            "Processing (13/25): NEJMcpc2402489.json...\n",
            "Processing (14/25): NEJMcpc2402490.json...\n",
            "Processing (15/25): NEJMcpc2402491.json...\n",
            "Processing (16/25): NEJMcpc2402492.json...\n",
            "Processing (17/25): NEJMcpc2402493.json...\n",
            "Processing (18/25): NEJMcpc2402496.json...\n",
            "Processing (19/25): NEJMcpc2402498.json...\n",
            "Processing (20/25): NEJMcpc2402499.json...\n",
            "Processing (21/25): NEJMcpc2402500.json...\n",
            "Processing (22/25): NEJMcpc2402504.json...\n",
            "Processing (23/25): NEJMcpc2402505.json...\n",
            "Processing (24/25): NEJMcpc2412511.json...\n",
            "Processing (25/25): NEJMcpc2412513.json...\n",
            "------------------------------\n",
            "Processing Summary:\n",
            " - Files successfully parsed and included: 25\n",
            " - Files skipped due to errors or missing keys: 0\n",
            " - Total JSON files found: 25\n",
            "------------------------------\n",
            "Successfully generated collated predictions file with ranks.\n",
            "Results saved to: /content/drive/MyDrive/grok_3_predictions.json\n",
            "\n",
            "Script finished.\n",
            "Please check the generated file: /content/drive/MyDrive/grok_3_predictions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collating All ChatGPT o4-mini-high Differential Diagnoses (Full Case Record)"
      ],
      "metadata": {
        "id": "gWdHSBHg8GuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Ensure necessary libraries are available ---\n",
        "\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "import glob # Useful for finding files matching a pattern\n",
        "\n",
        "# --- Function to find JSON files ---\n",
        "def find_json_files(directory):\n",
        "    \"\"\"Finds all .json files in a given directory, sorted alphabetically.\"\"\"\n",
        "    files = []\n",
        "    try:\n",
        "        if not os.path.isdir(directory):\n",
        "             print(f\"Error: Directory not found or not accessible: {directory}\")\n",
        "             return None\n",
        "        json_pattern = os.path.join(directory, '*.json')\n",
        "        files = sorted(glob.glob(json_pattern))\n",
        "        if not files:\n",
        "             print(f\"Warning: No .json files found in directory '{directory}'.\")\n",
        "        return files\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred listing or searching directory contents: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Function to parse a single prediction JSON file ---\n",
        "def parse_prediction_file(filepath):\n",
        "    \"\"\"\n",
        "    Parses a single ChatGPT prediction JSON file and extracts required fields,\n",
        "    assigning rank based on order.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the individual JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted 'case_id' and\n",
        "              'differential_diagnosis' list (with only 'diagnosis' and\n",
        "              'rank'), or None if parsing fails or required\n",
        "              keys are missing.\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(filepath)\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # --- Extract required top-level keys ---\n",
        "        case_id = data.get('case_id')\n",
        "        diff_diag_list = data.get('differential_diagnosis')\n",
        "\n",
        "        if case_id is None:\n",
        "            print(f\"Warning: Missing 'case_id' key in file: {filename}. Skipping.\")\n",
        "            return None\n",
        "        if diff_diag_list is None:\n",
        "            print(f\"Warning: Missing 'differential_diagnosis' key in file: {filename} (Case ID: {case_id}). Skipping.\")\n",
        "            return None\n",
        "        if not isinstance(diff_diag_list, list):\n",
        "             print(f\"Warning: 'differential_diagnosis' is not a list in file: {filename} (Case ID: {case_id}). Skipping.\")\n",
        "             return None\n",
        "\n",
        "        # --- Process the differential diagnosis list ---\n",
        "        processed_diff_diag = []\n",
        "        # Use enumerate to get index (for rank) and item\n",
        "        for index, item in enumerate(diff_diag_list):\n",
        "            if not isinstance(item, dict):\n",
        "                print(f\"Warning: Item in 'differential_diagnosis' is not a dictionary in file: {filename} (Case ID: {case_id}). Skipping item.\")\n",
        "                continue\n",
        "\n",
        "            diagnosis = item.get('diagnosis')\n",
        "            # We no longer need confidence_level = item.get('confidence_level')\n",
        "\n",
        "            if diagnosis is None:\n",
        "                print(f\"Warning: Missing 'diagnosis' key within an item in 'differential_diagnosis' in file: {filename} (Case ID: {case_id}). Skipping item.\")\n",
        "                continue\n",
        "\n",
        "            # Calculate rank (1-based index)\n",
        "            rank = index + 1\n",
        "\n",
        "            processed_diff_diag.append({\n",
        "                \"diagnosis\": diagnosis,\n",
        "                \"rank\": rank  # Use 'rank' key with the calculated rank\n",
        "            })\n",
        "\n",
        "        # Return the structured data for this case\n",
        "        return {\n",
        "            \"case_id\": case_id,\n",
        "            \"differential_diagnosis\": processed_diff_diag\n",
        "        }\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Invalid JSON structure in file: {filename}. Skipping.\")\n",
        "        return None\n",
        "    except FileNotFoundError:\n",
        "         print(f\"Error: File not found during processing: {filepath}. Skipping.\")\n",
        "         return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred processing file {filename}: {e}. Skipping.\")\n",
        "        return None\n",
        "\n",
        "# --- Configuration ---\n",
        "DEFAULT_OUTPUT_FILENAME = \"chatGPT_o4-mini-high_predictions.json\" # Updated filename\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Google Drive Mounting ---\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        DRIVE_ROOT = \"/content/drive/MyDrive/\"\n",
        "        print(f\"Google Drive mounted. Your 'My Drive' is at: {DRIVE_ROOT}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Get User Input for Folder Path ---\n",
        "    print(\"\\n--- Please specify the folder containing the 25 individual ChatGPT prediction JSON files ---\")\n",
        "    print(\"1. Use the 'Files' panel on the left to navigate to the folder.\")\n",
        "    print(\"2. Right-click on the folder.\")\n",
        "    print(\"3. Select 'Copy path'.\")\n",
        "    print(\"4. Paste the copied path below and press Enter.\")\n",
        "    print(\"-\" * 60)\n",
        "    input_directory_path = input(\"Paste the full path to the folder here: \")\n",
        "\n",
        "    # --- Clean and Validate Input Path ---\n",
        "    INPUT_DIRECTORY = input_directory_path.strip()\n",
        "    if not INPUT_DIRECTORY:\n",
        "        print(\"Error: No path provided. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Define Output Path (saving to the root of My Drive)\n",
        "    OUTPUT_JSON_FILE = os.path.join(DRIVE_ROOT, DEFAULT_OUTPUT_FILENAME)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Reading individual JSON files from: {INPUT_DIRECTORY}\")\n",
        "    print(f\"Collated output with ranks will be saved to: {OUTPUT_JSON_FILE}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # --- Check if Input Directory Exists ---\n",
        "    if not os.path.isdir(INPUT_DIRECTORY):\n",
        "        print(f\"Error: The path provided is not a valid directory: '{INPUT_DIRECTORY}'\")\n",
        "    else:\n",
        "        # --- Find and Process JSON Files ---\n",
        "        prediction_files = find_json_files(INPUT_DIRECTORY)\n",
        "        collated_predictions = []\n",
        "        files_processed_count = 0\n",
        "        files_skipped_count = 0\n",
        "        total_files_found = 0\n",
        "\n",
        "        if prediction_files: # Check if find_json_files returned a list\n",
        "            total_files_found = len(prediction_files)\n",
        "            print(f\"Found {total_files_found} JSON files. Processing...\")\n",
        "\n",
        "            for idx, file_path in enumerate(prediction_files):\n",
        "                filename = os.path.basename(file_path)\n",
        "                print(f\"Processing ({idx + 1}/{total_files_found}): {filename}...\")\n",
        "\n",
        "                # Parse the individual file\n",
        "                parsed_data = parse_prediction_file(file_path)\n",
        "\n",
        "                if parsed_data:\n",
        "                    collated_predictions.append(parsed_data)\n",
        "                    files_processed_count += 1\n",
        "                else:\n",
        "                    # Error/warning message already printed by parse_prediction_file\n",
        "                    files_skipped_count += 1\n",
        "\n",
        "            # --- Save Collated Results ---\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"Processing Summary:\")\n",
        "            print(f\" - Files successfully parsed and included: {files_processed_count}\")\n",
        "            print(f\" - Files skipped due to errors or missing keys: {files_skipped_count}\")\n",
        "            print(f\" - Total JSON files found: {total_files_found}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            if collated_predictions:\n",
        "                try:\n",
        "                    # Optional: Sort the final list by case_id if needed\n",
        "                    # collated_predictions.sort(key=lambda x: x.get('case_id', ''))\n",
        "\n",
        "                    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f_out:\n",
        "                        json.dump(collated_predictions, f_out, indent=2, ensure_ascii=False)\n",
        "                    print(f\"Successfully generated collated predictions file with ranks.\")\n",
        "                    print(f\"Results saved to: {OUTPUT_JSON_FILE}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nError writing final JSON output file: {e}\")\n",
        "            else:\n",
        "                print(\"\\nNo data was successfully parsed from any file. Output file not created.\")\n",
        "        else:\n",
        "             # Message handled by find_json_files or the initial directory check\n",
        "             print(\"Processing complete. No JSON files found in the specified directory.\")\n",
        "\n",
        "# Reminder\n",
        "print(\"\\nScript finished.\")\n",
        "if files_processed_count > 0:\n",
        "    print(f\"Please check the generated file: {OUTPUT_JSON_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLQPkTag8SDP",
        "outputId": "fa1b58ad-36d9-4153-a006-1840a3232834"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted. Your 'My Drive' is at: /content/drive/MyDrive/\n",
            "\n",
            "--- Please specify the folder containing the 25 individual ChatGPT prediction JSON files ---\n",
            "1. Use the 'Files' panel on the left to navigate to the folder.\n",
            "2. Right-click on the folder.\n",
            "3. Select 'Copy path'.\n",
            "4. Paste the copied path below and press Enter.\n",
            "------------------------------------------------------------\n",
            "Paste the full path to the folder here: /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/Full Case Records/Untitled folder\n",
            "------------------------------\n",
            "Reading individual JSON files from: /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/Full Case Records/Untitled folder\n",
            "Collated output with ranks will be saved to: /content/drive/MyDrive/chatGPT_o4-mini-high_predictions.json\n",
            "------------------------------\n",
            "Found 1 JSON files. Processing...\n",
            "Processing (1/1): Sample Conversation.json...\n",
            "------------------------------\n",
            "Processing Summary:\n",
            " - Files successfully parsed and included: 1\n",
            " - Files skipped due to errors or missing keys: 0\n",
            " - Total JSON files found: 1\n",
            "------------------------------\n",
            "Successfully generated collated predictions file with ranks.\n",
            "Results saved to: /content/drive/MyDrive/chatGPT_o4-mini-high_predictions.json\n",
            "\n",
            "Script finished.\n",
            "Please check the generated file: /content/drive/MyDrive/chatGPT_o4-mini-high_predictions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collating All Perplexity Research Differential Diagnoses (Full Case Record)"
      ],
      "metadata": {
        "id": "jQeMlRclDdsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Ensure necessary libraries are available ---\n",
        "\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "import glob # Useful for finding files matching a pattern\n",
        "\n",
        "# --- Function to find JSON files ---\n",
        "def find_json_files(directory):\n",
        "    \"\"\"Finds all .json files in a given directory, sorted alphabetically.\"\"\"\n",
        "    files = []\n",
        "    try:\n",
        "        if not os.path.isdir(directory):\n",
        "             print(f\"Error: Directory not found or not accessible: {directory}\")\n",
        "             return None\n",
        "        json_pattern = os.path.join(directory, '*.json')\n",
        "        files = sorted(glob.glob(json_pattern))\n",
        "        if not files:\n",
        "             print(f\"Warning: No .json files found in directory '{directory}'.\")\n",
        "        return files\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred listing or searching directory contents: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Function to parse a single prediction JSON file ---\n",
        "def parse_prediction_file(filepath):\n",
        "    \"\"\"\n",
        "    Parses a single Perplexity prediction JSON file and extracts required fields,\n",
        "    assigning rank based on order.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the individual JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted 'case_id' and\n",
        "              'differential_diagnosis' list (with only 'diagnosis' and\n",
        "              'rank'), or None if parsing fails or required\n",
        "              keys are missing.\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(filepath)\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # --- Extract required top-level keys ---\n",
        "        case_id = data.get('case_id')\n",
        "        diff_diag_list = data.get('differential_diagnosis')\n",
        "\n",
        "        if case_id is None:\n",
        "            print(f\"Warning: Missing 'case_id' key in file: {filename}. Skipping.\")\n",
        "            return None\n",
        "        if diff_diag_list is None:\n",
        "            print(f\"Warning: Missing 'differential_diagnosis' key in file: {filename} (Case ID: {case_id}). Skipping.\")\n",
        "            return None\n",
        "        if not isinstance(diff_diag_list, list):\n",
        "             print(f\"Warning: 'differential_diagnosis' is not a list in file: {filename} (Case ID: {case_id}). Skipping.\")\n",
        "             return None\n",
        "\n",
        "        # --- Process the differential diagnosis list ---\n",
        "        processed_diff_diag = []\n",
        "        # Use enumerate to get index (for rank) and item\n",
        "        for index, item in enumerate(diff_diag_list):\n",
        "            if not isinstance(item, dict):\n",
        "                print(f\"Warning: Item in 'differential_diagnosis' is not a dictionary in file: {filename} (Case ID: {case_id}). Skipping item.\")\n",
        "                continue\n",
        "\n",
        "            diagnosis = item.get('diagnosis')\n",
        "            # We no longer need confidence_level = item.get('confidence_level')\n",
        "\n",
        "            if diagnosis is None:\n",
        "                print(f\"Warning: Missing 'diagnosis' key within an item in 'differential_diagnosis' in file: {filename} (Case ID: {case_id}). Skipping item.\")\n",
        "                continue\n",
        "\n",
        "            # Calculate rank (1-based index)\n",
        "            rank = index + 1\n",
        "\n",
        "            processed_diff_diag.append({\n",
        "                \"diagnosis\": diagnosis,\n",
        "                \"rank\": rank  # Use 'rank' key with the calculated rank\n",
        "            })\n",
        "\n",
        "        # Return the structured data for this case\n",
        "        return {\n",
        "            \"case_id\": case_id,\n",
        "            \"differential_diagnosis\": processed_diff_diag\n",
        "        }\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Invalid JSON structure in file: {filename}. Skipping.\")\n",
        "        return None\n",
        "    except FileNotFoundError:\n",
        "         print(f\"Error: File not found during processing: {filepath}. Skipping.\")\n",
        "         return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred processing file {filename}: {e}. Skipping.\")\n",
        "        return None\n",
        "\n",
        "# --- Configuration ---\n",
        "DEFAULT_OUTPUT_FILENAME = \"perplexity_research_predictions.json\" # Updated filename\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Google Drive Mounting ---\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        DRIVE_ROOT = \"/content/drive/MyDrive/\"\n",
        "        print(f\"Google Drive mounted. Your 'My Drive' is at: {DRIVE_ROOT}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Get User Input for Folder Path ---\n",
        "    print(\"\\n--- Please specify the folder containing the 25 individual Perplexity prediction JSON files ---\")\n",
        "    print(\"1. Use the 'Files' panel on the left to navigate to the folder.\")\n",
        "    print(\"2. Right-click on the folder.\")\n",
        "    print(\"3. Select 'Copy path'.\")\n",
        "    print(\"4. Paste the copied path below and press Enter.\")\n",
        "    print(\"-\" * 60)\n",
        "    input_directory_path = input(\"Paste the full path to the folder here: \")\n",
        "\n",
        "    # --- Clean and Validate Input Path ---\n",
        "    INPUT_DIRECTORY = input_directory_path.strip()\n",
        "    if not INPUT_DIRECTORY:\n",
        "        print(\"Error: No path provided. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Define Output Path (saving to the root of My Drive)\n",
        "    OUTPUT_JSON_FILE = os.path.join(DRIVE_ROOT, DEFAULT_OUTPUT_FILENAME)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Reading individual JSON files from: {INPUT_DIRECTORY}\")\n",
        "    print(f\"Collated output with ranks will be saved to: {OUTPUT_JSON_FILE}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # --- Check if Input Directory Exists ---\n",
        "    if not os.path.isdir(INPUT_DIRECTORY):\n",
        "        print(f\"Error: The path provided is not a valid directory: '{INPUT_DIRECTORY}'\")\n",
        "    else:\n",
        "        # --- Find and Process JSON Files ---\n",
        "        prediction_files = find_json_files(INPUT_DIRECTORY)\n",
        "        collated_predictions = []\n",
        "        files_processed_count = 0\n",
        "        files_skipped_count = 0\n",
        "        total_files_found = 0\n",
        "\n",
        "        if prediction_files: # Check if find_json_files returned a list\n",
        "            total_files_found = len(prediction_files)\n",
        "            print(f\"Found {total_files_found} JSON files. Processing...\")\n",
        "\n",
        "            for idx, file_path in enumerate(prediction_files):\n",
        "                filename = os.path.basename(file_path)\n",
        "                print(f\"Processing ({idx + 1}/{total_files_found}): {filename}...\")\n",
        "\n",
        "                # Parse the individual file\n",
        "                parsed_data = parse_prediction_file(file_path)\n",
        "\n",
        "                if parsed_data:\n",
        "                    collated_predictions.append(parsed_data)\n",
        "                    files_processed_count += 1\n",
        "                else:\n",
        "                    # Error/warning message already printed by parse_prediction_file\n",
        "                    files_skipped_count += 1\n",
        "\n",
        "            # --- Save Collated Results ---\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"Processing Summary:\")\n",
        "            print(f\" - Files successfully parsed and included: {files_processed_count}\")\n",
        "            print(f\" - Files skipped due to errors or missing keys: {files_skipped_count}\")\n",
        "            print(f\" - Total JSON files found: {total_files_found}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            if collated_predictions:\n",
        "                try:\n",
        "                    # Optional: Sort the final list by case_id if needed\n",
        "                    # collated_predictions.sort(key=lambda x: x.get('case_id', ''))\n",
        "\n",
        "                    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f_out:\n",
        "                        json.dump(collated_predictions, f_out, indent=2, ensure_ascii=False)\n",
        "                    print(f\"Successfully generated collated predictions file with ranks.\")\n",
        "                    print(f\"Results saved to: {OUTPUT_JSON_FILE}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nError writing final JSON output file: {e}\")\n",
        "            else:\n",
        "                print(\"\\nNo data was successfully parsed from any file. Output file not created.\")\n",
        "        else:\n",
        "             # Message handled by find_json_files or the initial directory check\n",
        "             print(\"Processing complete. No JSON files found in the specified directory.\")\n",
        "\n",
        "# Reminder\n",
        "print(\"\\nScript finished.\")\n",
        "if files_processed_count > 0:\n",
        "    print(f\"Please check the generated file: {OUTPUT_JSON_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP-eHPpxDsRh",
        "outputId": "550f8466-f342-40e7-cdae-664b5602922c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted. Your 'My Drive' is at: /content/drive/MyDrive/\n",
            "\n",
            "--- Please specify the folder containing the 25 individual Perplexity prediction JSON files ---\n",
            "1. Use the 'Files' panel on the left to navigate to the folder.\n",
            "2. Right-click on the folder.\n",
            "3. Select 'Copy path'.\n",
            "4. Paste the copied path below and press Enter.\n",
            "------------------------------------------------------------\n",
            "Paste the full path to the folder here: /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/Perplexity Research Full Case Records\n",
            "------------------------------\n",
            "Reading individual JSON files from: /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/Perplexity Research Full Case Records\n",
            "Collated output with ranks will be saved to: /content/drive/MyDrive/perplexity_research_predictions.json\n",
            "------------------------------\n",
            "Found 25 JSON files. Processing...\n",
            "Processing (1/25): NEJMcpc2100279.json...\n",
            "Processing (2/25): NEJMcpc2300900.json...\n",
            "Processing (3/25): NEJMcpc2309383.json...\n",
            "Processing (4/25): NEJMcpc2309500.json...\n",
            "Processing (5/25): NEJMcpc2309726.json...\n",
            "Processing (6/25): NEJMcpc2312734.json...\n",
            "Processing (7/25): NEJMcpc2312735.json...\n",
            "Processing (8/25): NEJMcpc2402483.json...\n",
            "Processing (9/25): NEJMcpc2402485.json...\n",
            "Processing (10/25): NEJMcpc2402486.json...\n",
            "Processing (11/25): NEJMcpc2402487.json...\n",
            "Processing (12/25): NEJMcpc2402488.json...\n",
            "Processing (13/25): NEJMcpc2402489.json...\n",
            "Processing (14/25): NEJMcpc2402490.json...\n",
            "Processing (15/25): NEJMcpc2402491.json...\n",
            "Processing (16/25): NEJMcpc2402492.json...\n",
            "Processing (17/25): NEJMcpc2402493.json...\n",
            "Processing (18/25): NEJMcpc2402496.json...\n",
            "Processing (19/25): NEJMcpc2402498.json...\n",
            "Processing (20/25): NEJMcpc2402499.json...\n",
            "Processing (21/25): NEJMcpc2402500.json...\n",
            "Processing (22/25): NEJMcpc2402504.json...\n",
            "Processing (23/25): NEJMcpc2402505.json...\n",
            "Processing (24/25): NEJMcpc2412511.json...\n",
            "Processing (25/25): NEJMcpc2412513.json...\n",
            "------------------------------\n",
            "Processing Summary:\n",
            " - Files successfully parsed and included: 25\n",
            " - Files skipped due to errors or missing keys: 0\n",
            " - Total JSON files found: 25\n",
            "------------------------------\n",
            "Successfully generated collated predictions file with ranks.\n",
            "Results saved to: /content/drive/MyDrive/perplexity_research_predictions.json\n",
            "\n",
            "Script finished.\n",
            "Please check the generated file: /content/drive/MyDrive/perplexity_research_predictions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Mean Reciprocal Rank and Discounted Cumulative Gain for Gemini 2.5 Pro Responses"
      ],
      "metadata": {
        "id": "8XK5qcuVslDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Similarity using Embeddings"
      ],
      "metadata": {
        "id": "4pJ08uyFs13O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install sentence-transformers scikit-learn numpy -q\n",
        "\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util # For embeddings and similarity\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "def load_json_data(filepath):\n",
        "    \"\"\"Loads JSON data from a file.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from {filepath}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred loading {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_metrics_semantic(predictions_data, ground_truth_data, model, similarity_threshold=0.85, k=5):\n",
        "    \"\"\"\n",
        "    Calculates MRR, Mean NDCG@k, and Top-1 Accuracy using semantic similarity.\n",
        "\n",
        "    Args:\n",
        "        predictions_data (list): List of prediction dicts (case_id, differential_diagnosis list).\n",
        "        ground_truth_data (list): List of ground truth dicts (case_id, correct_diagnosis).\n",
        "        model (SentenceTransformer): The loaded sentence embedding model.\n",
        "        similarity_threshold (float): Cosine similarity threshold to consider diagnoses a match.\n",
        "        k (int): The cutoff for calculating NDCG.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the calculated metrics, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    if not predictions_data or not ground_truth_data:\n",
        "        print(\"Error: Input data is missing.\")\n",
        "        return None\n",
        "\n",
        "    # Convert lists to dictionaries keyed by case_id for efficient lookup\n",
        "    try:\n",
        "        predictions_dict = {item['case_id']: item['differential_diagnosis'] for item in predictions_data}\n",
        "        ground_truth_dict = {item['case_id']: item['correct_diagnosis'] for item in ground_truth_data}\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Missing expected key '{e}' while structuring data. Check JSON formats.\")\n",
        "        return None\n",
        "    except TypeError as e:\n",
        "        print(f\"Error: Problem accessing data, likely incorrect JSON structure: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    reciprocal_ranks = []\n",
        "    ndcg_scores = []\n",
        "    top1_correct_count = 0\n",
        "    processed_cases = 0\n",
        "    cases_with_match = 0\n",
        "\n",
        "    common_case_ids = set(predictions_dict.keys()) & set(ground_truth_dict.keys())\n",
        "\n",
        "    if not common_case_ids:\n",
        "        print(\"Error: No common case_ids found between the two files.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Processing {len(common_case_ids)} common cases...\")\n",
        "\n",
        "    for case_id in common_case_ids:\n",
        "        if case_id not in predictions_dict or case_id not in ground_truth_dict:\n",
        "            print(f\"Warning: Skipping case {case_id} - missing in one of the files.\")\n",
        "            continue\n",
        "\n",
        "        correct_diagnosis_text = ground_truth_dict[case_id]\n",
        "        predicted_diagnoses_list = predictions_dict[case_id]\n",
        "\n",
        "        if not correct_diagnosis_text:\n",
        "            print(f\"Warning: Empty correct_diagnosis for case {case_id}. Skipping.\")\n",
        "            continue\n",
        "        if not predicted_diagnoses_list:\n",
        "            print(f\"Warning: Empty differential_diagnosis list for case {case_id}. Assigning zero scores.\")\n",
        "            reciprocal_ranks.append(0)\n",
        "            ndcg_scores.append(0)\n",
        "            processed_cases += 1\n",
        "            continue\n",
        "\n",
        "        # Ensure predictions are sorted by rank (should be if generated correctly)\n",
        "        predicted_diagnoses_list.sort(key=lambda x: x.get('rank', float('inf')))\n",
        "        predicted_texts = [item.get('diagnosis', '') for item in predicted_diagnoses_list]\n",
        "        if not any(predicted_texts): # Check if all predicted texts are empty\n",
        "             print(f\"Warning: All predicted diagnosis texts are empty for case {case_id}. Assigning zero scores.\")\n",
        "             reciprocal_ranks.append(0)\n",
        "             ndcg_scores.append(0)\n",
        "             processed_cases += 1\n",
        "             continue\n",
        "\n",
        "\n",
        "        # --- Semantic Similarity Calculation ---\n",
        "        try:\n",
        "            # Generate embeddings\n",
        "            correct_embedding = model.encode(correct_diagnosis_text, convert_to_tensor=True)\n",
        "            predicted_embeddings = model.encode(predicted_texts, convert_to_tensor=True)\n",
        "\n",
        "            # Calculate cosine similarities\n",
        "            cosine_scores = util.cos_sim(correct_embedding, predicted_embeddings)[0] # Get the similarity scores list\n",
        "\n",
        "            # Find the rank of the *first* prediction exceeding the threshold\n",
        "            found_rank = 0\n",
        "            match_found_this_case = False\n",
        "            for i in range(len(predicted_diagnoses_list)):\n",
        "                similarity = cosine_scores[i].item() # Get similarity for the i-th prediction\n",
        "                # print(f\"  Case {case_id}, Rank {i+1}: '{predicted_texts[i]}' vs '{correct_diagnosis_text}' -> Sim: {similarity:.4f}\") # Debug print\n",
        "                if similarity >= similarity_threshold:\n",
        "                    found_rank = i + 1  # 1-based rank\n",
        "                    match_found_this_case = True\n",
        "                    # print(f\"    --> Match found at rank {found_rank}!\") # Debug print\n",
        "                    break # Stop at the first match in the ranked list\n",
        "\n",
        "            if match_found_this_case:\n",
        "                cases_with_match += 1\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error during embedding/similarity calculation for case {case_id}: {e}. Assigning zero scores.\")\n",
        "             found_rank = 0\n",
        "             match_found_this_case = False\n",
        "        # --- End Semantic Similarity ---\n",
        "\n",
        "\n",
        "        # --- Calculate Metrics based on found_rank ---\n",
        "        # RR\n",
        "        rr = 1 / found_rank if found_rank > 0 else 0\n",
        "        reciprocal_ranks.append(rr)\n",
        "\n",
        "        # DCG@k and NDCG@k\n",
        "        dcg = 0.0\n",
        "        # Check relevance based on the *single* match found by similarity\n",
        "        for i in range(min(k, len(predicted_diagnoses_list))):\n",
        "            rank_in_list = i + 1\n",
        "            # Relevance is 1 only if this item *is* the one identified as the match\n",
        "            relevance = 1 if rank_in_list == found_rank else 0\n",
        "            dcg += relevance / math.log2(rank_in_list + 1)\n",
        "\n",
        "        # IDCG@k is 1 if a match was possible (i.e., if found_rank > 0) and k >= found_rank\n",
        "        # Simpler: IDCG is the score if the best match was at rank 1.\n",
        "        idcg = 1.0 / math.log2(1 + 1) if found_rank > 0 else 0.0 # IDCG is 1 if a match exists\n",
        "\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "        ndcg_scores.append(ndcg)\n",
        "\n",
        "        # Top-1 Accuracy\n",
        "        if found_rank == 1:\n",
        "            top1_correct_count += 1\n",
        "\n",
        "        processed_cases += 1\n",
        "\n",
        "    # --- Aggregate Results ---\n",
        "    if processed_cases == 0:\n",
        "        print(\"Error: No cases were successfully processed.\")\n",
        "        return None\n",
        "\n",
        "    mean_mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "    mean_ndcg_at_k = np.mean(ndcg_scores) if ndcg_scores else 0\n",
        "    top_1_accuracy = top1_correct_count / processed_cases if processed_cases > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"total_cases_processed\": processed_cases,\n",
        "        \"cases_with_semantic_match_found\": cases_with_match,\n",
        "        \"similarity_threshold\": similarity_threshold,\n",
        "        \"embedding_model_used\": model.config.name_or_path if hasattr(model, 'config') and hasattr(model.config, 'name_or_path') else 'Unknown',\n",
        "        \"mrr\": mean_mrr,\n",
        "        f\"mean_ndcg@{k}\": mean_ndcg_at_k,\n",
        "        \"top_1_accuracy\": top_1_accuracy,\n",
        "        \"k_for_ndcg\": k\n",
        "    }\n",
        "\n",
        "# --- Configuration ---\n",
        "# Model choice: 'all-MiniLM-L6-v2' is fast and general purpose.\n",
        "# Consider 'emilyalsentzer/Bio_ClinicalBERT' or other biomedical models if available via sentence-transformers\n",
        "# or if you install transformers separately, but start with a general one.\n",
        "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "SIMILARITY_THRESHOLD = 0.75 # Adjust this threshold based on results (0.8-0.9 is common)\n",
        "NDCG_K = 5 # Evaluate NDCG up to rank 5\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Mount Drive ---\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        DRIVE_ROOT = \"/content/drive/MyDrive/\"\n",
        "        print(\"Google Drive mounted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Get File Paths ---\n",
        "    print(\"\\n--- Specify Input JSON File Paths ---\")\n",
        "    print(\"Use the Colab file browser (left panel) to copy the *full path* for each file.\")\n",
        "\n",
        "    ground_truth_file_path = input(\"1. Paste the full path to 'ground_truth_diagnoses.json': \").strip()\n",
        "    predictions_file_path = input(\"2. Paste the full path to 'gemini_2.5_pro_predictions.json': \").strip()\n",
        "\n",
        "    if not ground_truth_file_path or not predictions_file_path:\n",
        "        print(\"Error: One or both file paths were not provided. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Load Data ---\n",
        "    print(\"\\nLoading data...\")\n",
        "    ground_truth_data = load_json_data(ground_truth_file_path)\n",
        "    predictions_data = load_json_data(predictions_file_path)\n",
        "\n",
        "    if ground_truth_data is None or predictions_data is None:\n",
        "        print(\"Failed to load data. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Load Embedding Model ---\n",
        "    print(f\"\\nLoading sentence transformer model: {MODEL_NAME}...\")\n",
        "    try:\n",
        "        model = SentenceTransformer(MODEL_NAME)\n",
        "        print(\"Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading sentence transformer model: {e}\")\n",
        "        print(\"Please ensure the model name is correct and you have internet access.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Calculate Metrics ---\n",
        "    print(f\"\\nCalculating metrics using semantic similarity (Threshold: {SIMILARITY_THRESHOLD})...\")\n",
        "    results = calculate_metrics_semantic(\n",
        "        predictions_data,\n",
        "        ground_truth_data,\n",
        "        model,\n",
        "        similarity_threshold=SIMILARITY_THRESHOLD,\n",
        "        k=NDCG_K\n",
        "    )\n",
        "\n",
        "    # --- Display Results ---\n",
        "    if results:\n",
        "        print(\"\\n--- Evaluation Results ---\")\n",
        "        print(f\"Embedding Model: {results['embedding_model_used']}\")\n",
        "        print(f\"Similarity Threshold: {results['similarity_threshold']}\")\n",
        "        print(f\"Total Cases Processed: {results['total_cases_processed']}\")\n",
        "        print(f\"Cases Where a Match Was Found: {results['cases_with_semantic_match_found']} ({results['cases_with_semantic_match_found']/results['total_cases_processed']:.1%})\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"MRR (Mean Reciprocal Rank): {results['mrr']:.4f}\")\n",
        "\n",
        "        # Corrected NDCG print statement:\n",
        "        k_value = results['k_for_ndcg']\n",
        "        ndcg_key = f\"mean_ndcg@{k_value}\"\n",
        "        if ndcg_key in results:\n",
        "             print(f\"Mean NDCG@{k_value}: {results[ndcg_key]:.4f}\")\n",
        "        else:\n",
        "             print(f\"Mean NDCG@{k_value}: Key '{ndcg_key}' not found in results.\") # Error handling\n",
        "\n",
        "        print(f\"Top-1 Accuracy (Correct diagnosis is semantically matched at rank 1): {results['top_1_accuracy']:.2%}\")\n",
        "        print(\"------------------------\")\n",
        "    else:\n",
        "        print(\"\\nFailed to calculate metrics.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCTF3kX2We8c",
        "outputId": "bc6a993a-e758-4d06-af4a-8b03cfd61988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n",
            "\n",
            "--- Specify Input JSON File Paths ---\n",
            "Use the Colab file browser (left panel) to copy the *full path* for each file.\n",
            "1. Paste the full path to 'ground_truth_diagnoses.json': /content/drive/MyDrive/ground_truth_diagnoses.json\n",
            "2. Paste the full path to 'gemini_2.5_pro_predictions.json': /content/drive/MyDrive/gemini_2.5_pro_predictions_ranked.json\n",
            "\n",
            "Loading data...\n",
            "\n",
            "Loading sentence transformer model: all-MiniLM-L6-v2...\n",
            "Model loaded successfully.\n",
            "\n",
            "Calculating metrics using semantic similarity (Threshold: 0.75)...\n",
            "Processing 25 common cases...\n",
            "\n",
            "--- Evaluation Results ---\n",
            "Embedding Model: Unknown\n",
            "Similarity Threshold: 0.75\n",
            "Total Cases Processed: 25\n",
            "Cases Where a Match Was Found: 6 (24.0%)\n",
            "-------------------------\n",
            "MRR (Mean Reciprocal Rank): 0.2133\n",
            "Mean NDCG@5: 0.2200\n",
            "Top-1 Accuracy (Correct diagnosis is semantically matched at rank 1): 20.00%\n",
            "------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM-based Comparison"
      ],
      "metadata": {
        "id": "SaIO65E0tGoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install google-generativeai numpy -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import userdata # For securely getting the API key\n",
        "import google.generativeai as genai\n",
        "import glob # Useful for finding files matching a pattern\n",
        "\n",
        "# --- Function to load JSON data ---\n",
        "def load_json_data(filepath):\n",
        "    \"\"\"Loads JSON data from a file.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from {filepath}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred loading {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Function to ask Gemini for semantic match ---\n",
        "# (Using the version with temperature=0.0)\n",
        "def check_diagnosis_match_with_gemini(ground_truth_dx, predicted_dx, model, retries=2, delay=5):\n",
        "    \"\"\"\n",
        "    Asks the Gemini model if two diagnoses semantically match.\n",
        "    \"\"\"\n",
        "    if not predicted_dx:\n",
        "        return False\n",
        "\n",
        "    prompt = f\"\"\"Compare the following two medical diagnoses.\n",
        "Diagnosis 1 (Ground Truth): \"{ground_truth_dx}\"\n",
        "Diagnosis 2 (Prediction): \"{predicted_dx}\"\n",
        "\n",
        "Do these two diagnoses refer to essentially the same condition, a very close subtype, or is the prediction clearly encompassed within the ground truth, such that the prediction could be considered correct in this context?\n",
        "\n",
        "Answer ONLY with the word 'YES' or 'NO'.\n",
        "\"\"\"\n",
        "    response = None # Initialize response to None\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=genai.types.GenerationConfig(temperature=0.0)\n",
        "                )\n",
        "\n",
        "            cleaned_response = response.text.strip().upper().replace(\".\", \"\")\n",
        "            if cleaned_response == \"YES\":\n",
        "                return True\n",
        "            elif cleaned_response == \"NO\":\n",
        "                return False\n",
        "            else:\n",
        "                # Fallback check\n",
        "                if \"YES\" in cleaned_response:\n",
        "                     # print(f\"      Warning: LLM response unclear but contains YES ('{response.text}'). Treating as YES.\") # Optional debug\n",
        "                     return True\n",
        "                elif \"NO\" in cleaned_response:\n",
        "                     # print(f\"      Warning: LLM response unclear but contains NO ('{response.text}'). Treating as NO.\") # Optional debug\n",
        "                     return False\n",
        "                else:\n",
        "                     # print(f\"      Warning: LLM response was not clear YES/NO ('{response.text}'). Treating as NO.\") # Optional debug\n",
        "                     return False\n",
        "        except Exception as e:\n",
        "            block_reason = \"\"\n",
        "            # Try to access potential block reason safely\n",
        "            try:\n",
        "                 # Check if response exists and has the necessary attributes before accessing them\n",
        "                 if response and hasattr(response, 'prompt_feedback') and response.prompt_feedback and hasattr(response.prompt_feedback, 'block_reason') and response.prompt_feedback.block_reason:\n",
        "                      block_reason = f\" (Block Reason: {response.prompt_feedback.block_reason})\"\n",
        "            except AttributeError:\n",
        "                 pass # Ignore if feedback attributes don't exist or response is None\n",
        "\n",
        "            print(f\"      Error calling Gemini API (Attempt {attempt + 1}/{retries + 1}): {e}{block_reason}\")\n",
        "            if attempt < retries:\n",
        "                print(f\"      Retrying in {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(\"      Max retries reached. Treating as NO match.\")\n",
        "                return False\n",
        "    return False\n",
        "\n",
        "\n",
        "# --- Function to calculate metrics AND find matching ranks using LLM ---\n",
        "def calculate_metrics_and_ranks_llm(predictions_data, ground_truth_data, model, k=5):\n",
        "    \"\"\"\n",
        "    Calculates MRR, Mean NDCG@k, Top-1 Accuracy using LLM for matching,\n",
        "    AND stores details of the first match found for validation.\n",
        "\n",
        "    Args:\n",
        "        predictions_data (list): List of prediction dicts.\n",
        "        ground_truth_data (list): List of ground truth dicts.\n",
        "        model (genai.GenerativeModel): The initialized Gemini model.\n",
        "        k (int): The cutoff for calculating NDCG.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (dict: metrics, dict: individual match details) or (None, None)\n",
        "    \"\"\"\n",
        "    if not predictions_data or not ground_truth_data:\n",
        "        print(\"Error: Input data is missing.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        predictions_dict = {item['case_id']: item['differential_diagnosis'] for item in predictions_data}\n",
        "        ground_truth_dict = {item['case_id']: item['correct_diagnosis'] for item in ground_truth_data}\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Missing expected key '{e}' while structuring data.\")\n",
        "        return None, None\n",
        "    except TypeError as e:\n",
        "        print(f\"Error: Problem accessing data, likely incorrect JSON structure: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    reciprocal_ranks = []\n",
        "    ndcg_scores = []\n",
        "    top1_correct_count = 0\n",
        "    processed_cases = 0\n",
        "    cases_with_match = 0\n",
        "    individual_match_details = {} # To store rank and text for validation\n",
        "\n",
        "    common_case_ids = sorted(list(set(predictions_dict.keys()) & set(ground_truth_dict.keys())))\n",
        "\n",
        "    if not common_case_ids:\n",
        "        print(\"Error: No common case_ids found.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\nProcessing {len(common_case_ids)} common cases using LLM ({model.model_name})...\")\n",
        "    print(\"This will take time...\\n\")\n",
        "\n",
        "    for case_id in common_case_ids:\n",
        "        print(f\"--- Processing Case: {case_id} ---\")\n",
        "        if case_id not in predictions_dict or case_id not in ground_truth_dict:\n",
        "            print(\"  Skipped - Data Missing in one of the files.\")\n",
        "            individual_match_details[case_id] = {\"status\": \"Skipped - Data Missing\"}\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "\n",
        "        correct_diagnosis_text = ground_truth_dict[case_id]\n",
        "        predicted_diagnoses_list = predictions_dict[case_id]\n",
        "\n",
        "        if not correct_diagnosis_text:\n",
        "            print(\"  Skipped - No Ground Truth diagnosis text.\")\n",
        "            individual_match_details[case_id] = {\"status\": \"Skipped - No Ground Truth\"}\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "        if not predicted_diagnoses_list:\n",
        "            print(\"  No Match Possible (Prediction list is empty). Assigning zero scores.\")\n",
        "            reciprocal_ranks.append(0)\n",
        "            ndcg_scores.append(0)\n",
        "            individual_match_details[case_id] = {\"status\": \"No Match Found (Empty Predictions)\"}\n",
        "            processed_cases += 1\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "\n",
        "        # Ensure predictions are sorted by rank\n",
        "        predicted_diagnoses_list.sort(key=lambda x: x.get('rank', float('inf')))\n",
        "\n",
        "        # --- LLM Comparison Loop ---\n",
        "        found_rank = 0\n",
        "        first_match_details = {}\n",
        "        for i, prediction_item in enumerate(predicted_diagnoses_list):\n",
        "            current_rank = i + 1\n",
        "            predicted_text = prediction_item.get('diagnosis', '')\n",
        "            print(f\"  Rank {current_rank}: Comparing...\") # Keep output concise\n",
        "\n",
        "            # Call LLM to check match\n",
        "            is_match = check_diagnosis_match_with_gemini(correct_diagnosis_text, predicted_text, model)\n",
        "            time.sleep(1.1) # IMPORTANT: Rate limiting\n",
        "\n",
        "            if is_match:\n",
        "                print(f\"    --> Match found by LLM at rank {current_rank}!\")\n",
        "                found_rank = current_rank\n",
        "                first_match_details = {\n",
        "                    \"rank\": found_rank,\n",
        "                    \"ground_truth\": correct_diagnosis_text,\n",
        "                    \"prediction\": predicted_text\n",
        "                }\n",
        "                break # Stop at the first match\n",
        "\n",
        "        # --- Store results for this case ---\n",
        "        if found_rank > 0:\n",
        "             individual_match_details[case_id] = first_match_details\n",
        "             cases_with_match += 1\n",
        "        else:\n",
        "             individual_match_details[case_id] = {\"status\": \"No Match Found\"}\n",
        "             print(\"  No Match Found for this case.\")\n",
        "\n",
        "\n",
        "        # --- Calculate Metrics based on found_rank ---\n",
        "        rr = 1 / found_rank if found_rank > 0 else 0\n",
        "        reciprocal_ranks.append(rr)\n",
        "\n",
        "        dcg = 0.0\n",
        "        for i in range(min(k, len(predicted_diagnoses_list))):\n",
        "            rank_in_list = i + 1\n",
        "            relevance = 1 if rank_in_list == found_rank else 0\n",
        "            dcg += relevance / math.log2(rank_in_list + 1)\n",
        "\n",
        "        idcg = 1.0 / math.log2(1 + 1) if found_rank > 0 else 0.0\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "        ndcg_scores.append(ndcg)\n",
        "\n",
        "        if found_rank == 1:\n",
        "            top1_correct_count += 1\n",
        "\n",
        "        processed_cases += 1\n",
        "        print(\"-\" * 30) # Separator between cases\n",
        "\n",
        "    # --- Aggregate Results ---\n",
        "    if processed_cases == 0:\n",
        "        print(\"Error: No cases were successfully processed.\")\n",
        "        return None, None\n",
        "\n",
        "    mean_mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "    mean_ndcg_at_k = np.mean(ndcg_scores) if ndcg_scores else 0\n",
        "    top_1_accuracy = top1_correct_count / processed_cases if processed_cases > 0 else 0\n",
        "\n",
        "    metrics = {\n",
        "        \"total_cases_processed\": processed_cases,\n",
        "        \"cases_with_llm_match_found\": cases_with_match,\n",
        "        \"llm_model_used\": model.model_name,\n",
        "        \"mrr\": mean_mrr,\n",
        "        f\"mean_ndcg@{k}\": mean_ndcg_at_k,\n",
        "        \"top_1_accuracy\": top_1_accuracy,\n",
        "        \"k_for_ndcg\": k\n",
        "    }\n",
        "    return metrics, individual_match_details\n",
        "\n",
        "# --- Configuration ---\n",
        "LLM_MODEL_NAME = 'gemini-2.5-pro-preview-03-25' # Your specified model\n",
        "NDCG_K = 5 # Rank cutoff for NDCG calculation\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Mount Drive ---\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        DRIVE_ROOT = \"/content/drive/MyDrive/\"\n",
        "        print(\"Google Drive mounted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Configure Gemini API ---\n",
        "    try:\n",
        "        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "        if not GOOGLE_API_KEY:\n",
        "            raise ValueError(\"API Key not found in Colab Secrets\")\n",
        "        genai.configure(api_key=GOOGLE_API_KEY)\n",
        "        print(\"Gemini API configured.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring Gemini API: {e}\")\n",
        "        print(\"Please ensure you have set the 'GOOGLE_API_KEY' secret in Colab.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Initialize the Gemini Model ---\n",
        "    try:\n",
        "        print(f\"Initializing Gemini model: {LLM_MODEL_NAME}...\")\n",
        "        model = genai.GenerativeModel(LLM_MODEL_NAME)\n",
        "        print(\"Model initialized.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Gemini model: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Get File Paths ---\n",
        "    print(\"\\n--- Specify Input JSON File Paths ---\")\n",
        "    print(\"Use the Colab file browser (left panel) to copy the *full path* for each file.\")\n",
        "    ground_truth_file_path = input(\"1. Paste the full path to 'ground_truth_diagnoses.json': \").strip()\n",
        "    predictions_file_path = input(\"2. Paste the full path to 'gemini_2.5_pro_predictions.json': \").strip() # Use the ranked one\n",
        "\n",
        "    if not ground_truth_file_path or not predictions_file_path:\n",
        "        print(\"Error: One or both file paths were not provided. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Load Data ---\n",
        "    print(\"\\nLoading data...\")\n",
        "    ground_truth_data = load_json_data(ground_truth_file_path)\n",
        "    predictions_data = load_json_data(predictions_file_path)\n",
        "\n",
        "    if ground_truth_data is None or predictions_data is None:\n",
        "        print(\"Failed to load data. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Calculate Metrics and Find Ranks using LLM ---\n",
        "    metrics_results, individual_match_info = calculate_metrics_and_ranks_llm(\n",
        "        predictions_data,\n",
        "        ground_truth_data,\n",
        "        model,\n",
        "        k=NDCG_K\n",
        "    )\n",
        "\n",
        "    # --- Display Individual Match Details (for Validation) ---\n",
        "    if individual_match_info:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"--- Individual Case Match Details (Rank of First LLM Match) ---\")\n",
        "        print(\"=\"*60)\n",
        "        for case_id, details in individual_match_info.items():\n",
        "            print(f\"{case_id}:\")\n",
        "            if \"rank\" in details: # Check if a match was found\n",
        "                print(f\"  Rank {details['rank']}: '{details['ground_truth']}' vs '{details['prediction']}'\")\n",
        "            else:\n",
        "                print(f\"  {details.get('status', 'Unknown Status')}\") # Print status like 'No Match Found' or 'Skipped'\n",
        "            print(\"-\" * 30)\n",
        "    else:\n",
        "        print(\"\\nCould not retrieve individual match details.\")\n",
        "\n",
        "\n",
        "    # --- Display Overall Metrics ---\n",
        "    if metrics_results:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"--- Overall Evaluation Results (LLM-based) ---\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"LLM Model Used: {metrics_results['llm_model_used']}\")\n",
        "        print(f\"Total Cases Processed: {metrics_results['total_cases_processed']}\")\n",
        "        print(f\"Cases Where LLM Found a Match: {metrics_results['cases_with_llm_match_found']} ({metrics_results['cases_with_llm_match_found']/metrics_results['total_cases_processed']:.1%})\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"MRR (Mean Reciprocal Rank): {metrics_results['mrr']:.4f}\")\n",
        "\n",
        "        k_value = metrics_results['k_for_ndcg']\n",
        "        ndcg_key = f\"mean_ndcg@{k_value}\"\n",
        "        if ndcg_key in metrics_results:\n",
        "             print(f\"Mean NDCG@{k_value}: {metrics_results[ndcg_key]:.4f}\")\n",
        "        else:\n",
        "             print(f\"Mean NDCG@{k_value}: Key '{ndcg_key}' not found in results.\")\n",
        "\n",
        "        print(f\"Top-1 Accuracy (LLM match at rank 1): {metrics_results['top_1_accuracy']:.2%}\")\n",
        "        print(\"=\"*60)\n",
        "    else:\n",
        "        print(\"\\nFailed to calculate overall metrics.\")\n",
        "\n",
        "    print(\"\\nScript finished.\")"
      ],
      "metadata": {
        "id": "YOpqCJG5lT4j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8dfc63bf-4337-4fe6-eee5-f2af56fcefac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n",
            "Gemini API configured.\n",
            "Initializing Gemini model: gemini-2.5-pro-preview-03-25...\n",
            "Model initialized.\n",
            "\n",
            "--- Specify Input JSON File Paths ---\n",
            "Use the Colab file browser (left panel) to copy the *full path* for each file.\n",
            "1. Paste the full path to 'ground_truth_diagnoses.json': /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/ground_truth_diagnoses.json\n",
            "2. Paste the full path to 'gemini_2.5_pro_predictions.json': /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/gemini_2.5_pro_predictions.json\n",
            "\n",
            "Loading data...\n",
            "\n",
            "Processing 25 common cases using LLM (models/gemini-2.5-pro-preview-03-25)...\n",
            "This will take time...\n",
            "\n",
            "--- Processing Case: NEJMcpc2100279 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2300900 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2309383 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2309500 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2309726 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2312734 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2312735 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402483 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402485 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402486 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "    --> Match found by LLM at rank 3!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402487 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402488 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402489 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402490 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402491 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402492 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402493 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402496 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "    --> Match found by LLM at rank 4!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402498 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402499 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402500 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402504 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "    --> Match found by LLM at rank 4!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402505 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2412511 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2412513 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "\n",
            "============================================================\n",
            "--- Individual Case Match Details (Rank of First LLM Match) ---\n",
            "============================================================\n",
            "NEJMcpc2100279:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2300900:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2309383:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2309500:\n",
            "  Rank 1: 'Sweets syndrome.' vs 'Sweet's Syndrome (Acute Febrile Neutrophilic Dermatosis)'\n",
            "------------------------------\n",
            "NEJMcpc2309726:\n",
            "  Rank 1: 'Nutritional optic neuropathy due to multiple nutritional deficits, including vitamin A, copper, and zinc deficiencies.' vs 'Nutritional Optic Neuropathy (Vitamin A Deficiency)'\n",
            "------------------------------\n",
            "NEJMcpc2312734:\n",
            "  Rank 1: 'Myasthenia gravis.' vs 'Myasthenia Gravis (MG)'\n",
            "------------------------------\n",
            "NEJMcpc2312735:\n",
            "  Rank 1: 'Postpartum obsessivecompulsive disorder.' vs 'Postpartum Obsessive-Compulsive Disorder (OCD)'\n",
            "------------------------------\n",
            "NEJMcpc2402483:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402485:\n",
            "  Rank 1: '24-Hydroxylase deficiency due to a homozygous CYP24A1 variant.' vs 'Idiopathic Infantile Hypercalcemia (IIH) / CYP24A1 or SLC34A1 Mutation'\n",
            "------------------------------\n",
            "NEJMcpc2402486:\n",
            "  Rank 3: 'RosaiDorfmanDestombes disease.' vs 'Rosai-Dorfman Disease (RDD)'\n",
            "------------------------------\n",
            "NEJMcpc2402487:\n",
            "  Rank 1: 'Intralobar bronchopulmonary sequestration.' vs 'Congenital Pulmonary Airway Malformation (CPAM) / Bronchopulmonary Sequestration (BPS)'\n",
            "------------------------------\n",
            "NEJMcpc2402488:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402489:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402490:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402491:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402492:\n",
            "  Rank 1: 'Legionella infection complicated by rhabdomyolysis.' vs 'Severe Legionellosis (Legionella pneumophila infection)'\n",
            "------------------------------\n",
            "NEJMcpc2402493:\n",
            "  Rank 1: 'Icteric leptospirosis.' vs 'Severe Leptospirosis (Weil's Disease)'\n",
            "------------------------------\n",
            "NEJMcpc2402496:\n",
            "  Rank 4: 'Brain abscess due to infection with Listeria monocytogenes.' vs 'Infectious Lesion (Toxoplasmosis, Fungal, Abscess, PML)'\n",
            "------------------------------\n",
            "NEJMcpc2402498:\n",
            "  Rank 1: 'Postpartum nephrotic syndrome due to focal segmental glomerulosclerosis.' vs 'Nephrotic Syndrome (likely Minimal Change Disease or FSGS)'\n",
            "------------------------------\n",
            "NEJMcpc2402499:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402500:\n",
            "  Rank 1: 'Psychotic disorder due to a general medical condition (postictal psychosis).' vs 'Postictal Psychosis (PIP)'\n",
            "------------------------------\n",
            "NEJMcpc2402504:\n",
            "  Rank 4: 'Cryptococcal meningoencephalitis.' vs 'Meningitis / Encephalitis'\n",
            "------------------------------\n",
            "NEJMcpc2402505:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2412511:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2412513:\n",
            "  Rank 1: 'Sarcoidosis (Lfgrens syndrome).' vs 'Sarcoidosis (presenting acutely as Lfgren's Syndrome variant)'\n",
            "------------------------------\n",
            "\n",
            "============================================================\n",
            "--- Overall Evaluation Results (LLM-based) ---\n",
            "============================================================\n",
            "LLM Model Used: models/gemini-2.5-pro-preview-03-25\n",
            "Total Cases Processed: 25\n",
            "Cases Where LLM Found a Match: 14 (56.0%)\n",
            "-------------------------\n",
            "MRR (Mean Reciprocal Rank): 0.4733\n",
            "Mean NDCG@5: 0.4945\n",
            "Top-1 Accuracy (LLM match at rank 1): 44.00%\n",
            "============================================================\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Mean Reciprocal Rank and Discounted Cumulative Gain for Grok 3 Responses"
      ],
      "metadata": {
        "id": "d8ivL26My-_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM-based Comparison"
      ],
      "metadata": {
        "id": "PvMNEvX8zpZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install google-generativeai numpy -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import userdata # For securely getting the API key\n",
        "import google.generativeai as genai\n",
        "import glob # Useful for finding files matching a pattern\n",
        "\n",
        "# --- Function to load JSON data ---\n",
        "def load_json_data(filepath):\n",
        "    \"\"\"Loads JSON data from a file.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from {filepath}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred loading {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Function to ask Gemini for semantic match ---\n",
        "# (Using the version with temperature=0.0)\n",
        "def check_diagnosis_match_with_gemini(ground_truth_dx, predicted_dx, model, retries=2, delay=5):\n",
        "    \"\"\"\n",
        "    Asks the Gemini model if two diagnoses semantically match.\n",
        "    \"\"\"\n",
        "    if not predicted_dx:\n",
        "        return False\n",
        "\n",
        "    prompt = f\"\"\"Compare the following two medical diagnoses.\n",
        "Diagnosis 1 (Ground Truth): \"{ground_truth_dx}\"\n",
        "Diagnosis 2 (Prediction): \"{predicted_dx}\"\n",
        "\n",
        "Do these two diagnoses refer to essentially the same condition, a very close subtype, or is the prediction clearly encompassed within the ground truth, such that the prediction could be considered correct in this context?\n",
        "\n",
        "Answer ONLY with the word 'YES' or 'NO'.\n",
        "\"\"\"\n",
        "    response = None # Initialize response to None\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=genai.types.GenerationConfig(temperature=0.0)\n",
        "                )\n",
        "\n",
        "            cleaned_response = response.text.strip().upper().replace(\".\", \"\")\n",
        "            if cleaned_response == \"YES\":\n",
        "                return True\n",
        "            elif cleaned_response == \"NO\":\n",
        "                return False\n",
        "            else:\n",
        "                # Fallback check\n",
        "                if \"YES\" in cleaned_response:\n",
        "                     # print(f\"      Warning: LLM response unclear but contains YES ('{response.text}'). Treating as YES.\") # Optional debug\n",
        "                     return True\n",
        "                elif \"NO\" in cleaned_response:\n",
        "                     # print(f\"      Warning: LLM response unclear but contains NO ('{response.text}'). Treating as NO.\") # Optional debug\n",
        "                     return False\n",
        "                else:\n",
        "                     # print(f\"      Warning: LLM response was not clear YES/NO ('{response.text}'). Treating as NO.\") # Optional debug\n",
        "                     return False\n",
        "        except Exception as e:\n",
        "            block_reason = \"\"\n",
        "            # Try to access potential block reason safely\n",
        "            try:\n",
        "                 # Check if response exists and has the necessary attributes before accessing them\n",
        "                 if response and hasattr(response, 'prompt_feedback') and response.prompt_feedback and hasattr(response.prompt_feedback, 'block_reason') and response.prompt_feedback.block_reason:\n",
        "                      block_reason = f\" (Block Reason: {response.prompt_feedback.block_reason})\"\n",
        "            except AttributeError:\n",
        "                 pass # Ignore if feedback attributes don't exist or response is None\n",
        "\n",
        "            print(f\"      Error calling Gemini API (Attempt {attempt + 1}/{retries + 1}): {e}{block_reason}\")\n",
        "            if attempt < retries:\n",
        "                print(f\"      Retrying in {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(\"      Max retries reached. Treating as NO match.\")\n",
        "                return False\n",
        "    return False\n",
        "\n",
        "\n",
        "# --- Function to calculate metrics AND find matching ranks using LLM ---\n",
        "def calculate_metrics_and_ranks_llm(predictions_data, ground_truth_data, model, k=5):\n",
        "    \"\"\"\n",
        "    Calculates MRR, Mean NDCG@k, Top-1 Accuracy using LLM for matching,\n",
        "    AND stores details of the first match found for validation.\n",
        "\n",
        "    Args:\n",
        "        predictions_data (list): List of prediction dicts.\n",
        "        ground_truth_data (list): List of ground truth dicts.\n",
        "        model (genai.GenerativeModel): The initialized Gemini model.\n",
        "        k (int): The cutoff for calculating NDCG.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (dict: metrics, dict: individual match details) or (None, None)\n",
        "    \"\"\"\n",
        "    if not predictions_data or not ground_truth_data:\n",
        "        print(\"Error: Input data is missing.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        predictions_dict = {item['case_id']: item['differential_diagnosis'] for item in predictions_data}\n",
        "        ground_truth_dict = {item['case_id']: item['correct_diagnosis'] for item in ground_truth_data}\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Missing expected key '{e}' while structuring data.\")\n",
        "        return None, None\n",
        "    except TypeError as e:\n",
        "        print(f\"Error: Problem accessing data, likely incorrect JSON structure: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    reciprocal_ranks = []\n",
        "    ndcg_scores = []\n",
        "    top1_correct_count = 0\n",
        "    processed_cases = 0\n",
        "    cases_with_match = 0\n",
        "    individual_match_details = {} # To store rank and text for validation\n",
        "\n",
        "    common_case_ids = sorted(list(set(predictions_dict.keys()) & set(ground_truth_dict.keys())))\n",
        "\n",
        "    if not common_case_ids:\n",
        "        print(\"Error: No common case_ids found.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\nProcessing {len(common_case_ids)} common cases using LLM ({model.model_name})...\")\n",
        "    print(\"This will take time...\\n\")\n",
        "\n",
        "    for case_id in common_case_ids:\n",
        "        print(f\"--- Processing Case: {case_id} ---\")\n",
        "        if case_id not in predictions_dict or case_id not in ground_truth_dict:\n",
        "            print(\"  Skipped - Data Missing in one of the files.\")\n",
        "            individual_match_details[case_id] = {\"status\": \"Skipped - Data Missing\"}\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "\n",
        "        correct_diagnosis_text = ground_truth_dict[case_id]\n",
        "        predicted_diagnoses_list = predictions_dict[case_id]\n",
        "\n",
        "        if not correct_diagnosis_text:\n",
        "            print(\"  Skipped - No Ground Truth diagnosis text.\")\n",
        "            individual_match_details[case_id] = {\"status\": \"Skipped - No Ground Truth\"}\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "        if not predicted_diagnoses_list:\n",
        "            print(\"  No Match Possible (Prediction list is empty). Assigning zero scores.\")\n",
        "            reciprocal_ranks.append(0)\n",
        "            ndcg_scores.append(0)\n",
        "            individual_match_details[case_id] = {\"status\": \"No Match Found (Empty Predictions)\"}\n",
        "            processed_cases += 1\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "\n",
        "        # Ensure predictions are sorted by rank\n",
        "        predicted_diagnoses_list.sort(key=lambda x: x.get('rank', float('inf')))\n",
        "\n",
        "        # --- LLM Comparison Loop ---\n",
        "        found_rank = 0\n",
        "        first_match_details = {}\n",
        "        for i, prediction_item in enumerate(predicted_diagnoses_list):\n",
        "            current_rank = i + 1\n",
        "            predicted_text = prediction_item.get('diagnosis', '')\n",
        "            print(f\"  Rank {current_rank}: Comparing...\") # Keep output concise\n",
        "\n",
        "            # Call LLM to check match\n",
        "            is_match = check_diagnosis_match_with_gemini(correct_diagnosis_text, predicted_text, model)\n",
        "            time.sleep(1.1) # IMPORTANT: Rate limiting\n",
        "\n",
        "            if is_match:\n",
        "                print(f\"    --> Match found by LLM at rank {current_rank}!\")\n",
        "                found_rank = current_rank\n",
        "                first_match_details = {\n",
        "                    \"rank\": found_rank,\n",
        "                    \"ground_truth\": correct_diagnosis_text,\n",
        "                    \"prediction\": predicted_text\n",
        "                }\n",
        "                break # Stop at the first match\n",
        "\n",
        "        # --- Store results for this case ---\n",
        "        if found_rank > 0:\n",
        "             individual_match_details[case_id] = first_match_details\n",
        "             cases_with_match += 1\n",
        "        else:\n",
        "             individual_match_details[case_id] = {\"status\": \"No Match Found\"}\n",
        "             print(\"  No Match Found for this case.\")\n",
        "\n",
        "\n",
        "        # --- Calculate Metrics based on found_rank ---\n",
        "        rr = 1 / found_rank if found_rank > 0 else 0\n",
        "        reciprocal_ranks.append(rr)\n",
        "\n",
        "        dcg = 0.0\n",
        "        for i in range(min(k, len(predicted_diagnoses_list))):\n",
        "            rank_in_list = i + 1\n",
        "            relevance = 1 if rank_in_list == found_rank else 0\n",
        "            dcg += relevance / math.log2(rank_in_list + 1)\n",
        "\n",
        "        idcg = 1.0 / math.log2(1 + 1) if found_rank > 0 else 0.0\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "        ndcg_scores.append(ndcg)\n",
        "\n",
        "        if found_rank == 1:\n",
        "            top1_correct_count += 1\n",
        "\n",
        "        processed_cases += 1\n",
        "        print(\"-\" * 30) # Separator between cases\n",
        "\n",
        "    # --- Aggregate Results ---\n",
        "    if processed_cases == 0:\n",
        "        print(\"Error: No cases were successfully processed.\")\n",
        "        return None, None\n",
        "\n",
        "    mean_mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "    mean_ndcg_at_k = np.mean(ndcg_scores) if ndcg_scores else 0\n",
        "    top_1_accuracy = top1_correct_count / processed_cases if processed_cases > 0 else 0\n",
        "\n",
        "    metrics = {\n",
        "        \"total_cases_processed\": processed_cases,\n",
        "        \"cases_with_llm_match_found\": cases_with_match,\n",
        "        \"llm_model_used\": model.model_name,\n",
        "        \"mrr\": mean_mrr,\n",
        "        f\"mean_ndcg@{k}\": mean_ndcg_at_k,\n",
        "        \"top_1_accuracy\": top_1_accuracy,\n",
        "        \"k_for_ndcg\": k\n",
        "    }\n",
        "    return metrics, individual_match_details\n",
        "\n",
        "# --- Configuration ---\n",
        "LLM_MODEL_NAME = 'gemini-2.5-pro-preview-03-25' # Your specified model\n",
        "NDCG_K = 5 # Rank cutoff for NDCG calculation\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Mount Drive ---\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        DRIVE_ROOT = \"/content/drive/MyDrive/\"\n",
        "        print(\"Google Drive mounted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Configure Gemini API ---\n",
        "    try:\n",
        "        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "        if not GOOGLE_API_KEY:\n",
        "            raise ValueError(\"API Key not found in Colab Secrets\")\n",
        "        genai.configure(api_key=GOOGLE_API_KEY)\n",
        "        print(\"Gemini API configured.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring Gemini API: {e}\")\n",
        "        print(\"Please ensure you have set the 'GOOGLE_API_KEY' secret in Colab.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Initialize the Gemini Model ---\n",
        "    try:\n",
        "        print(f\"Initializing Gemini model: {LLM_MODEL_NAME}...\")\n",
        "        model = genai.GenerativeModel(LLM_MODEL_NAME)\n",
        "        print(\"Model initialized.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Gemini model: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Get File Paths ---\n",
        "    print(\"\\n--- Specify Input JSON File Paths ---\")\n",
        "    print(\"Use the Colab file browser (left panel) to copy the *full path* for each file.\")\n",
        "    ground_truth_file_path = input(\"1. Paste the full path to 'ground_truth_diagnoses.json': \").strip()\n",
        "    predictions_file_path = input(\"2. Paste the full path to 'grok_3_predictions.json': \").strip() # Use the ranked one\n",
        "\n",
        "    if not ground_truth_file_path or not predictions_file_path:\n",
        "        print(\"Error: One or both file paths were not provided. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Load Data ---\n",
        "    print(\"\\nLoading data...\")\n",
        "    ground_truth_data = load_json_data(ground_truth_file_path)\n",
        "    predictions_data = load_json_data(predictions_file_path)\n",
        "\n",
        "    if ground_truth_data is None or predictions_data is None:\n",
        "        print(\"Failed to load data. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Calculate Metrics and Find Ranks using LLM ---\n",
        "    metrics_results, individual_match_info = calculate_metrics_and_ranks_llm(\n",
        "        predictions_data,\n",
        "        ground_truth_data,\n",
        "        model,\n",
        "        k=NDCG_K\n",
        "    )\n",
        "\n",
        "    # --- Display Individual Match Details (for Validation) ---\n",
        "    if individual_match_info:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"--- Individual Case Match Details (Rank of First LLM Match) ---\")\n",
        "        print(\"=\"*60)\n",
        "        for case_id, details in individual_match_info.items():\n",
        "            print(f\"{case_id}:\")\n",
        "            if \"rank\" in details: # Check if a match was found\n",
        "                print(f\"  Rank {details['rank']}: '{details['ground_truth']}' vs '{details['prediction']}'\")\n",
        "            else:\n",
        "                print(f\"  {details.get('status', 'Unknown Status')}\") # Print status like 'No Match Found' or 'Skipped'\n",
        "            print(\"-\" * 30)\n",
        "    else:\n",
        "        print(\"\\nCould not retrieve individual match details.\")\n",
        "\n",
        "\n",
        "    # --- Display Overall Metrics ---\n",
        "    if metrics_results:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"--- Overall Evaluation Results (LLM-based) ---\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"LLM Model Used: {metrics_results['llm_model_used']}\")\n",
        "        print(f\"Total Cases Processed: {metrics_results['total_cases_processed']}\")\n",
        "        print(f\"Cases Where LLM Found a Match: {metrics_results['cases_with_llm_match_found']} ({metrics_results['cases_with_llm_match_found']/metrics_results['total_cases_processed']:.1%})\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"MRR (Mean Reciprocal Rank): {metrics_results['mrr']:.4f}\")\n",
        "\n",
        "        k_value = metrics_results['k_for_ndcg']\n",
        "        ndcg_key = f\"mean_ndcg@{k_value}\"\n",
        "        if ndcg_key in metrics_results:\n",
        "             print(f\"Mean NDCG@{k_value}: {metrics_results[ndcg_key]:.4f}\")\n",
        "        else:\n",
        "             print(f\"Mean NDCG@{k_value}: Key '{ndcg_key}' not found in results.\")\n",
        "\n",
        "        print(f\"Top-1 Accuracy (LLM match at rank 1): {metrics_results['top_1_accuracy']:.2%}\")\n",
        "        print(\"=\"*60)\n",
        "    else:\n",
        "        print(\"\\nFailed to calculate overall metrics.\")\n",
        "\n",
        "    print(\"\\nScript finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "avClz6gyy_h3",
        "outputId": "6b131769-aea5-4f0c-e539-a626b1dd5848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n",
            "Gemini API configured.\n",
            "Initializing Gemini model: gemini-2.5-pro-preview-03-25...\n",
            "Model initialized.\n",
            "\n",
            "--- Specify Input JSON File Paths ---\n",
            "Use the Colab file browser (left panel) to copy the *full path* for each file.\n",
            "1. Paste the full path to 'ground_truth_diagnoses.json': /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/ground_truth_diagnoses.json\n",
            "2. Paste the full path to 'grok_3_predictions.json': /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/grok_3_predictions.json\n",
            "\n",
            "Loading data...\n",
            "\n",
            "Processing 25 common cases using LLM (models/gemini-2.5-pro-preview-03-25)...\n",
            "This will take time...\n",
            "\n",
            "--- Processing Case: NEJMcpc2100279 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2300900 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2309383 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2309500 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2309726 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "    --> Match found by LLM at rank 5!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2312734 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2312735 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402483 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "    --> Match found by LLM at rank 2!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402485 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402486 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "    --> Match found by LLM at rank 5!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402487 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402488 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402489 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402490 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402491 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402492 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402493 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402496 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402498 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "    --> Match found by LLM at rank 2!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402499 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402500 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402504 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402505 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "    --> Match found by LLM at rank 2!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2412511 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "    --> Match found by LLM at rank 4!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2412513 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "\n",
            "============================================================\n",
            "--- Individual Case Match Details (Rank of First LLM Match) ---\n",
            "============================================================\n",
            "NEJMcpc2100279:\n",
            "  Rank 1: 'Infective endocarditis due to Haemophilus parainfluenzae.' vs 'Infective Endocarditis'\n",
            "------------------------------\n",
            "NEJMcpc2300900:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2309383:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2309500:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2309726:\n",
            "  Rank 5: 'Nutritional optic neuropathy due to multiple nutritional deficits, including vitamin A, copper, and zinc deficiencies.' vs 'Nutritional Deficiency'\n",
            "------------------------------\n",
            "NEJMcpc2312734:\n",
            "  Rank 1: 'Myasthenia gravis.' vs 'Myasthenia Gravis'\n",
            "------------------------------\n",
            "NEJMcpc2312735:\n",
            "  Rank 1: 'Postpartum obsessivecompulsive disorder.' vs 'Postpartum Obsessive-Compulsive Disorder'\n",
            "------------------------------\n",
            "NEJMcpc2402483:\n",
            "  Rank 2: 'Overlap syndrome of rheumatoid arthritis and systemic lupus erythematosus complicated by pro-liferative lupus nephritis, superimposed on amy- loid A amyloidosis.' vs 'AA Amyloidosis'\n",
            "------------------------------\n",
            "NEJMcpc2402485:\n",
            "  Rank 1: '24-Hydroxylase deficiency due to a homozygous CYP24A1 variant.' vs 'Idiopathic Infantile Hypercalcemia (Type 2)'\n",
            "------------------------------\n",
            "NEJMcpc2402486:\n",
            "  Rank 5: 'RosaiDorfmanDestombes disease.' vs 'Histiocytic Disorder'\n",
            "------------------------------\n",
            "NEJMcpc2402487:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402488:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402489:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402490:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402491:\n",
            "  Rank 1: 'Bronchopneumonia, most likely due to influenza.' vs 'Community-Acquired Pneumonia'\n",
            "------------------------------\n",
            "NEJMcpc2402492:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402493:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402496:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402498:\n",
            "  Rank 2: 'Postpartum nephrotic syndrome due to focal segmental glomerulosclerosis.' vs 'Focal Segmental Glomerulosclerosis (FSGS)'\n",
            "------------------------------\n",
            "NEJMcpc2402499:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402500:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402504:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402505:\n",
            "  Rank 2: 'Postmyocardial infarction (scar-related) reentrant ventricular tachycardia.' vs 'Ventricular Tachycardia'\n",
            "------------------------------\n",
            "NEJMcpc2412511:\n",
            "  Rank 4: 'Posterior reversible encephalopathy syndrome due to sickle cell disease.' vs 'Seizure'\n",
            "------------------------------\n",
            "NEJMcpc2412513:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "\n",
            "============================================================\n",
            "--- Overall Evaluation Results (LLM-based) ---\n",
            "============================================================\n",
            "LLM Model Used: models/gemini-2.5-pro-preview-03-25\n",
            "Total Cases Processed: 25\n",
            "Cases Where LLM Found a Match: 11 (44.0%)\n",
            "-------------------------\n",
            "MRR (Mean Reciprocal Rank): 0.2860\n",
            "Mean NDCG@5: 0.3239\n",
            "Top-1 Accuracy (LLM match at rank 1): 20.00%\n",
            "============================================================\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Mean Reciprocal Rank and Discounted Cumulative Gain for ChatGPT o4-mini-high Responses"
      ],
      "metadata": {
        "id": "A2g9fRQQCjIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install google-generativeai numpy -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import userdata # For securely getting the API key\n",
        "import google.generativeai as genai\n",
        "import glob # Useful for finding files matching a pattern\n",
        "\n",
        "# --- Function to load JSON data ---\n",
        "def load_json_data(filepath):\n",
        "    \"\"\"Loads JSON data from a file.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from {filepath}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred loading {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Function to ask Gemini for semantic match ---\n",
        "# (Using the version with temperature=0.0)\n",
        "def check_diagnosis_match_with_gemini(ground_truth_dx, predicted_dx, model, retries=2, delay=5):\n",
        "    \"\"\"\n",
        "    Asks the Gemini model if two diagnoses semantically match.\n",
        "    \"\"\"\n",
        "    if not predicted_dx:\n",
        "        return False\n",
        "\n",
        "    prompt = f\"\"\"Compare the following two medical diagnoses.\n",
        "Diagnosis 1 (Ground Truth): \"{ground_truth_dx}\"\n",
        "Diagnosis 2 (Prediction): \"{predicted_dx}\"\n",
        "\n",
        "Do these two diagnoses refer to essentially the same condition, a very close subtype, or is the prediction clearly encompassed within the ground truth, such that the prediction could be considered correct in this context?\n",
        "\n",
        "Answer ONLY with the word 'YES' or 'NO'.\n",
        "\"\"\"\n",
        "    response = None # Initialize response to None\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=genai.types.GenerationConfig(temperature=0.0)\n",
        "                )\n",
        "\n",
        "            cleaned_response = response.text.strip().upper().replace(\".\", \"\")\n",
        "            if cleaned_response == \"YES\":\n",
        "                return True\n",
        "            elif cleaned_response == \"NO\":\n",
        "                return False\n",
        "            else:\n",
        "                # Fallback check\n",
        "                if \"YES\" in cleaned_response:\n",
        "                     # print(f\"      Warning: LLM response unclear but contains YES ('{response.text}'). Treating as YES.\") # Optional debug\n",
        "                     return True\n",
        "                elif \"NO\" in cleaned_response:\n",
        "                     # print(f\"      Warning: LLM response unclear but contains NO ('{response.text}'). Treating as NO.\") # Optional debug\n",
        "                     return False\n",
        "                else:\n",
        "                     # print(f\"      Warning: LLM response was not clear YES/NO ('{response.text}'). Treating as NO.\") # Optional debug\n",
        "                     return False\n",
        "        except Exception as e:\n",
        "            block_reason = \"\"\n",
        "            # Try to access potential block reason safely\n",
        "            try:\n",
        "                 # Check if response exists and has the necessary attributes before accessing them\n",
        "                 if response and hasattr(response, 'prompt_feedback') and response.prompt_feedback and hasattr(response.prompt_feedback, 'block_reason') and response.prompt_feedback.block_reason:\n",
        "                      block_reason = f\" (Block Reason: {response.prompt_feedback.block_reason})\"\n",
        "            except AttributeError:\n",
        "                 pass # Ignore if feedback attributes don't exist or response is None\n",
        "\n",
        "            print(f\"      Error calling Gemini API (Attempt {attempt + 1}/{retries + 1}): {e}{block_reason}\")\n",
        "            if attempt < retries:\n",
        "                print(f\"      Retrying in {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(\"      Max retries reached. Treating as NO match.\")\n",
        "                return False\n",
        "    return False\n",
        "\n",
        "\n",
        "# --- Function to calculate metrics AND find matching ranks using LLM ---\n",
        "def calculate_metrics_and_ranks_llm(predictions_data, ground_truth_data, model, k=5):\n",
        "    \"\"\"\n",
        "    Calculates MRR, Mean NDCG@k, Top-1 Accuracy using LLM for matching,\n",
        "    AND stores details of the first match found for validation.\n",
        "\n",
        "    Args:\n",
        "        predictions_data (list): List of prediction dicts.\n",
        "        ground_truth_data (list): List of ground truth dicts.\n",
        "        model (genai.GenerativeModel): The initialized Gemini model.\n",
        "        k (int): The cutoff for calculating NDCG.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (dict: metrics, dict: individual match details) or (None, None)\n",
        "    \"\"\"\n",
        "    if not predictions_data or not ground_truth_data:\n",
        "        print(\"Error: Input data is missing.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        predictions_dict = {item['case_id']: item['differential_diagnosis'] for item in predictions_data}\n",
        "        ground_truth_dict = {item['case_id']: item['correct_diagnosis'] for item in ground_truth_data}\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Missing expected key '{e}' while structuring data.\")\n",
        "        return None, None\n",
        "    except TypeError as e:\n",
        "        print(f\"Error: Problem accessing data, likely incorrect JSON structure: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    reciprocal_ranks = []\n",
        "    ndcg_scores = []\n",
        "    top1_correct_count = 0\n",
        "    processed_cases = 0\n",
        "    cases_with_match = 0\n",
        "    individual_match_details = {} # To store rank and text for validation\n",
        "\n",
        "    common_case_ids = sorted(list(set(predictions_dict.keys()) & set(ground_truth_dict.keys())))\n",
        "\n",
        "    if not common_case_ids:\n",
        "        print(\"Error: No common case_ids found.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\nProcessing {len(common_case_ids)} common cases using LLM ({model.model_name})...\")\n",
        "    print(\"This will take time...\\n\")\n",
        "\n",
        "    for case_id in common_case_ids:\n",
        "        print(f\"--- Processing Case: {case_id} ---\")\n",
        "        if case_id not in predictions_dict or case_id not in ground_truth_dict:\n",
        "            print(\"  Skipped - Data Missing in one of the files.\")\n",
        "            individual_match_details[case_id] = {\"status\": \"Skipped - Data Missing\"}\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "\n",
        "        correct_diagnosis_text = ground_truth_dict[case_id]\n",
        "        predicted_diagnoses_list = predictions_dict[case_id]\n",
        "\n",
        "        if not correct_diagnosis_text:\n",
        "            print(\"  Skipped - No Ground Truth diagnosis text.\")\n",
        "            individual_match_details[case_id] = {\"status\": \"Skipped - No Ground Truth\"}\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "        if not predicted_diagnoses_list:\n",
        "            print(\"  No Match Possible (Prediction list is empty). Assigning zero scores.\")\n",
        "            reciprocal_ranks.append(0)\n",
        "            ndcg_scores.append(0)\n",
        "            individual_match_details[case_id] = {\"status\": \"No Match Found (Empty Predictions)\"}\n",
        "            processed_cases += 1\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "\n",
        "        # Ensure predictions are sorted by rank\n",
        "        predicted_diagnoses_list.sort(key=lambda x: x.get('rank', float('inf')))\n",
        "\n",
        "        # --- LLM Comparison Loop ---\n",
        "        found_rank = 0\n",
        "        first_match_details = {}\n",
        "        for i, prediction_item in enumerate(predicted_diagnoses_list):\n",
        "            current_rank = i + 1\n",
        "            predicted_text = prediction_item.get('diagnosis', '')\n",
        "            print(f\"  Rank {current_rank}: Comparing...\") # Keep output concise\n",
        "\n",
        "            # Call LLM to check match\n",
        "            is_match = check_diagnosis_match_with_gemini(correct_diagnosis_text, predicted_text, model)\n",
        "            time.sleep(1.1) # IMPORTANT: Rate limiting\n",
        "\n",
        "            if is_match:\n",
        "                print(f\"    --> Match found by LLM at rank {current_rank}!\")\n",
        "                found_rank = current_rank\n",
        "                first_match_details = {\n",
        "                    \"rank\": found_rank,\n",
        "                    \"ground_truth\": correct_diagnosis_text,\n",
        "                    \"prediction\": predicted_text\n",
        "                }\n",
        "                break # Stop at the first match\n",
        "\n",
        "        # --- Store results for this case ---\n",
        "        if found_rank > 0:\n",
        "             individual_match_details[case_id] = first_match_details\n",
        "             cases_with_match += 1\n",
        "        else:\n",
        "             individual_match_details[case_id] = {\"status\": \"No Match Found\"}\n",
        "             print(\"  No Match Found for this case.\")\n",
        "\n",
        "\n",
        "        # --- Calculate Metrics based on found_rank ---\n",
        "        rr = 1 / found_rank if found_rank > 0 else 0\n",
        "        reciprocal_ranks.append(rr)\n",
        "\n",
        "        dcg = 0.0\n",
        "        for i in range(min(k, len(predicted_diagnoses_list))):\n",
        "            rank_in_list = i + 1\n",
        "            relevance = 1 if rank_in_list == found_rank else 0\n",
        "            dcg += relevance / math.log2(rank_in_list + 1)\n",
        "\n",
        "        idcg = 1.0 / math.log2(1 + 1) if found_rank > 0 else 0.0\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "        ndcg_scores.append(ndcg)\n",
        "\n",
        "        if found_rank == 1:\n",
        "            top1_correct_count += 1\n",
        "\n",
        "        processed_cases += 1\n",
        "        print(\"-\" * 30) # Separator between cases\n",
        "\n",
        "    # --- Aggregate Results ---\n",
        "    if processed_cases == 0:\n",
        "        print(\"Error: No cases were successfully processed.\")\n",
        "        return None, None\n",
        "\n",
        "    mean_mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "    mean_ndcg_at_k = np.mean(ndcg_scores) if ndcg_scores else 0\n",
        "    top_1_accuracy = top1_correct_count / processed_cases if processed_cases > 0 else 0\n",
        "\n",
        "    metrics = {\n",
        "        \"total_cases_processed\": processed_cases,\n",
        "        \"cases_with_llm_match_found\": cases_with_match,\n",
        "        \"llm_model_used\": model.model_name,\n",
        "        \"mrr\": mean_mrr,\n",
        "        f\"mean_ndcg@{k}\": mean_ndcg_at_k,\n",
        "        \"top_1_accuracy\": top_1_accuracy,\n",
        "        \"k_for_ndcg\": k\n",
        "    }\n",
        "    return metrics, individual_match_details\n",
        "\n",
        "# --- Configuration ---\n",
        "LLM_MODEL_NAME = 'gemini-2.5-pro-preview-03-25' # Your specified model\n",
        "NDCG_K = 5 # Rank cutoff for NDCG calculation\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Mount Drive ---\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        DRIVE_ROOT = \"/content/drive/MyDrive/\"\n",
        "        print(\"Google Drive mounted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Configure Gemini API ---\n",
        "    try:\n",
        "        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "        if not GOOGLE_API_KEY:\n",
        "            raise ValueError(\"API Key not found in Colab Secrets\")\n",
        "        genai.configure(api_key=GOOGLE_API_KEY)\n",
        "        print(\"Gemini API configured.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring Gemini API: {e}\")\n",
        "        print(\"Please ensure you have set the 'GOOGLE_API_KEY' secret in Colab.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Initialize the Gemini Model ---\n",
        "    try:\n",
        "        print(f\"Initializing Gemini model: {LLM_MODEL_NAME}...\")\n",
        "        model = genai.GenerativeModel(LLM_MODEL_NAME)\n",
        "        print(\"Model initialized.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Gemini model: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Get File Paths ---\n",
        "    print(\"\\n--- Specify Input JSON File Paths ---\")\n",
        "    print(\"Use the Colab file browser (left panel) to copy the *full path* for each file.\")\n",
        "    ground_truth_file_path = input(\"1. Paste the full path to 'ground_truth_diagnoses.json': \").strip()\n",
        "    predictions_file_path = input(\"2. Paste the full path to 'chatGPT_o4-mini-high_predictions.json': \").strip() # Use the ranked one\n",
        "\n",
        "    if not ground_truth_file_path or not predictions_file_path:\n",
        "        print(\"Error: One or both file paths were not provided. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Load Data ---\n",
        "    print(\"\\nLoading data...\")\n",
        "    ground_truth_data = load_json_data(ground_truth_file_path)\n",
        "    predictions_data = load_json_data(predictions_file_path)\n",
        "\n",
        "    if ground_truth_data is None or predictions_data is None:\n",
        "        print(\"Failed to load data. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Calculate Metrics and Find Ranks using LLM ---\n",
        "    metrics_results, individual_match_info = calculate_metrics_and_ranks_llm(\n",
        "        predictions_data,\n",
        "        ground_truth_data,\n",
        "        model,\n",
        "        k=NDCG_K\n",
        "    )\n",
        "\n",
        "    # --- Display Individual Match Details (for Validation) ---\n",
        "    if individual_match_info:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"--- Individual Case Match Details (Rank of First LLM Match) ---\")\n",
        "        print(\"=\"*60)\n",
        "        for case_id, details in individual_match_info.items():\n",
        "            print(f\"{case_id}:\")\n",
        "            if \"rank\" in details: # Check if a match was found\n",
        "                print(f\"  Rank {details['rank']}: '{details['ground_truth']}' vs '{details['prediction']}'\")\n",
        "            else:\n",
        "                print(f\"  {details.get('status', 'Unknown Status')}\") # Print status like 'No Match Found' or 'Skipped'\n",
        "            print(\"-\" * 30)\n",
        "    else:\n",
        "        print(\"\\nCould not retrieve individual match details.\")\n",
        "\n",
        "\n",
        "    # --- Display Overall Metrics ---\n",
        "    if metrics_results:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"--- Overall Evaluation Results (LLM-based) ---\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"LLM Model Used: {metrics_results['llm_model_used']}\")\n",
        "        print(f\"Total Cases Processed: {metrics_results['total_cases_processed']}\")\n",
        "        print(f\"Cases Where LLM Found a Match: {metrics_results['cases_with_llm_match_found']} ({metrics_results['cases_with_llm_match_found']/metrics_results['total_cases_processed']:.1%})\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"MRR (Mean Reciprocal Rank): {metrics_results['mrr']:.4f}\")\n",
        "\n",
        "        k_value = metrics_results['k_for_ndcg']\n",
        "        ndcg_key = f\"mean_ndcg@{k_value}\"\n",
        "        if ndcg_key in metrics_results:\n",
        "             print(f\"Mean NDCG@{k_value}: {metrics_results[ndcg_key]:.4f}\")\n",
        "        else:\n",
        "             print(f\"Mean NDCG@{k_value}: Key '{ndcg_key}' not found in results.\")\n",
        "\n",
        "        print(f\"Top-1 Accuracy (LLM match at rank 1): {metrics_results['top_1_accuracy']:.2%}\")\n",
        "        print(\"=\"*60)\n",
        "    else:\n",
        "        print(\"\\nFailed to calculate overall metrics.\")\n",
        "\n",
        "    print(\"\\nScript finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_9VDZ4JVCpP0",
        "outputId": "d965c0ba-8176-4209-80d0-07f1e661e790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n",
            "Gemini API configured.\n",
            "Initializing Gemini model: gemini-2.5-pro-preview-03-25...\n",
            "Model initialized.\n",
            "\n",
            "--- Specify Input JSON File Paths ---\n",
            "Use the Colab file browser (left panel) to copy the *full path* for each file.\n",
            "1. Paste the full path to 'ground_truth_diagnoses.json': /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/ground_truth_diagnoses.json\n",
            "2. Paste the full path to 'chatGPT_o4-mini-high_predictions.json': /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/chatGPT_o4-mini-high_predictions.json\n",
            "\n",
            "Loading data...\n",
            "\n",
            "Processing 25 common cases using LLM (models/gemini-2.5-pro-preview-03-25)...\n",
            "This will take time...\n",
            "\n",
            "--- Processing Case: NEJMcpc2100279 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  Rank 6: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2300900 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "    --> Match found by LLM at rank 2!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2309383 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "    --> Match found by LLM at rank 5!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2309500 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "    --> Match found by LLM at rank 5!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2309726 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "    --> Match found by LLM at rank 4!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2312734 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  Rank 6: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2312735 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402483 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402485 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402486 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  Rank 6: Comparing...\n",
            "  Rank 7: Comparing...\n",
            "  Rank 8: Comparing...\n",
            "  Rank 9: Comparing...\n",
            "  Rank 10: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402487 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  Rank 6: Comparing...\n",
            "  Rank 7: Comparing...\n",
            "  Rank 8: Comparing...\n",
            "  Rank 9: Comparing...\n",
            "  Rank 10: Comparing...\n",
            "    --> Match found by LLM at rank 10!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402488 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402489 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  Rank 6: Comparing...\n",
            "  Rank 7: Comparing...\n",
            "  Rank 8: Comparing...\n",
            "  Rank 9: Comparing...\n",
            "  Rank 10: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402490 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402491 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "    --> Match found by LLM at rank 2!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402492 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "    --> Match found by LLM at rank 2!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402493 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402496 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  Rank 6: Comparing...\n",
            "  Rank 7: Comparing...\n",
            "  Rank 8: Comparing...\n",
            "  Rank 9: Comparing...\n",
            "  Rank 10: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402498 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "    --> Match found by LLM at rank 3!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402499 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402500 ---\n",
            "  Rank 1: Comparing...\n",
            "    --> Match found by LLM at rank 1!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402504 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  Rank 6: Comparing...\n",
            "  Rank 7: Comparing...\n",
            "  Rank 8: Comparing...\n",
            "  Rank 9: Comparing...\n",
            "    --> Match found by LLM at rank 9!\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2402505 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  Rank 6: Comparing...\n",
            "  Rank 7: Comparing...\n",
            "  Rank 8: Comparing...\n",
            "  Rank 9: Comparing...\n",
            "  Rank 10: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2412511 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  Rank 6: Comparing...\n",
            "  Rank 7: Comparing...\n",
            "  Rank 8: Comparing...\n",
            "  Rank 9: Comparing...\n",
            "  Rank 10: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "--- Processing Case: NEJMcpc2412513 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  Rank 6: Comparing...\n",
            "  Rank 7: Comparing...\n",
            "  Rank 8: Comparing...\n",
            "  Rank 9: Comparing...\n",
            "  Rank 10: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "\n",
            "============================================================\n",
            "--- Individual Case Match Details (Rank of First LLM Match) ---\n",
            "============================================================\n",
            "NEJMcpc2100279:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2300900:\n",
            "  Rank 2: 'AL amyloidosis.' vs 'Restrictive cardiomyopathy due to cardiac amyloidosis'\n",
            "------------------------------\n",
            "NEJMcpc2309383:\n",
            "  Rank 5: 'Common variable immunodeficiency.' vs 'Common variable immunodeficiency (CVID) with granulomatous disease'\n",
            "------------------------------\n",
            "NEJMcpc2309500:\n",
            "  Rank 5: 'Sweets syndrome.' vs 'Sweet's syndrome (acute febrile neutrophilic dermatosis)'\n",
            "------------------------------\n",
            "NEJMcpc2309726:\n",
            "  Rank 4: 'Nutritional optic neuropathy due to multiple nutritional deficits, including vitamin A, copper, and zinc deficiencies.' vs 'Nutritional optic neuropathy (vitamin deficiency)'\n",
            "------------------------------\n",
            "NEJMcpc2312734:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2312735:\n",
            "  Rank 1: 'Postpartum obsessivecompulsive disorder.' vs 'Postpartum Obsessive-Compulsive Disorder (OCD)'\n",
            "------------------------------\n",
            "NEJMcpc2402483:\n",
            "  Rank 1: 'Overlap syndrome of rheumatoid arthritis and systemic lupus erythematosus complicated by pro-liferative lupus nephritis, superimposed on amy- loid A amyloidosis.' vs 'AA (Secondary) Amyloidosis due to Rheumatoid Arthritis'\n",
            "------------------------------\n",
            "NEJMcpc2402485:\n",
            "  Rank 1: '24-Hydroxylase deficiency due to a homozygous CYP24A1 variant.' vs 'Idiopathic Infantile Hypercalcemia (CYP24A1 mutation)'\n",
            "------------------------------\n",
            "NEJMcpc2402486:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402487:\n",
            "  Rank 10: 'Intralobar bronchopulmonary sequestration.' vs 'Bronchopulmonary Sequestration'\n",
            "------------------------------\n",
            "NEJMcpc2402488:\n",
            "  Rank 1: 'Paraneoplastic encephalomyelitis due to smallcell lung carcinoma and concurrent cerebral amyloid angiopathy.' vs 'Paraneoplastic Limbic Encephalitis (Anti-Hu associated with Small Cell Lung Cancer)'\n",
            "------------------------------\n",
            "NEJMcpc2402489:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402490:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402491:\n",
            "  Rank 2: 'Bronchopneumonia, most likely due to influenza.' vs 'Viral Influenza or Other Atypical Pneumonitis'\n",
            "------------------------------\n",
            "NEJMcpc2402492:\n",
            "  Rank 2: 'Legionella infection complicated by rhabdomyolysis.' vs 'Legionnaires Disease (Legionella Pneumonia)'\n",
            "------------------------------\n",
            "NEJMcpc2402493:\n",
            "  Rank 1: 'Icteric leptospirosis.' vs 'Leptospirosis (Weils disease)'\n",
            "------------------------------\n",
            "NEJMcpc2402496:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2402498:\n",
            "  Rank 3: 'Postpartum nephrotic syndrome due to focal segmental glomerulosclerosis.' vs 'Focal segmental glomerulosclerosis (FSGS)'\n",
            "------------------------------\n",
            "NEJMcpc2402499:\n",
            "  Rank 1: 'Trichobezoar.' vs 'Gastric trichobezoar (Rapunzel syndrome)'\n",
            "------------------------------\n",
            "NEJMcpc2402500:\n",
            "  Rank 1: 'Psychotic disorder due to a general medical condition (postictal psychosis).' vs 'Postictal psychosis'\n",
            "------------------------------\n",
            "NEJMcpc2402504:\n",
            "  Rank 9: 'Cryptococcal meningoencephalitis.' vs 'Cryptococcal meningitis'\n",
            "------------------------------\n",
            "NEJMcpc2402505:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2412511:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "NEJMcpc2412513:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "\n",
            "============================================================\n",
            "--- Overall Evaluation Results (LLM-based) ---\n",
            "============================================================\n",
            "LLM Model Used: models/gemini-2.5-pro-preview-03-25\n",
            "Total Cases Processed: 25\n",
            "Cases Where LLM Found a Match: 16 (64.0%)\n",
            "-------------------------\n",
            "MRR (Mean Reciprocal Rank): 0.3878\n",
            "Mean NDCG@5: 0.4239\n",
            "Top-1 Accuracy (LLM match at rank 1): 28.00%\n",
            "============================================================\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Mean Reciprocal Rank and Discounted Cumulative Gain for Perplexity Research Responses"
      ],
      "metadata": {
        "id": "Rf6gkH9eIqGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install google-generativeai numpy -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import userdata # For securely getting the API key\n",
        "import google.generativeai as genai\n",
        "import glob # Useful for finding files matching a pattern\n",
        "\n",
        "# --- Function to load JSON data ---\n",
        "def load_json_data(filepath):\n",
        "    \"\"\"Loads JSON data from a file.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from {filepath}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred loading {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Function to ask Gemini for semantic match ---\n",
        "# (Using the version with temperature=0.0)\n",
        "def check_diagnosis_match_with_gemini(ground_truth_dx, predicted_dx, model, retries=2, delay=5):\n",
        "    \"\"\"\n",
        "    Asks the Gemini model if two diagnoses semantically match.\n",
        "    \"\"\"\n",
        "    if not predicted_dx:\n",
        "        return False\n",
        "\n",
        "    prompt = f\"\"\"Compare the following two medical diagnoses.\n",
        "Diagnosis 1 (Ground Truth): \"{ground_truth_dx}\"\n",
        "Diagnosis 2 (Prediction): \"{predicted_dx}\"\n",
        "\n",
        "Do these two diagnoses refer to essentially the same condition, a very close subtype, or is the prediction clearly encompassed within the ground truth, such that the prediction could be considered correct in this context?\n",
        "\n",
        "Answer ONLY with the word 'YES' or 'NO'.\n",
        "\"\"\"\n",
        "    response = None # Initialize response to None\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=genai.types.GenerationConfig(temperature=0.0)\n",
        "                )\n",
        "\n",
        "            cleaned_response = response.text.strip().upper().replace(\".\", \"\")\n",
        "            if cleaned_response == \"YES\":\n",
        "                return True\n",
        "            elif cleaned_response == \"NO\":\n",
        "                return False\n",
        "            else:\n",
        "                # Fallback check\n",
        "                if \"YES\" in cleaned_response:\n",
        "                     # print(f\"      Warning: LLM response unclear but contains YES ('{response.text}'). Treating as YES.\") # Optional debug\n",
        "                     return True\n",
        "                elif \"NO\" in cleaned_response:\n",
        "                     # print(f\"      Warning: LLM response unclear but contains NO ('{response.text}'). Treating as NO.\") # Optional debug\n",
        "                     return False\n",
        "                else:\n",
        "                     # print(f\"      Warning: LLM response was not clear YES/NO ('{response.text}'). Treating as NO.\") # Optional debug\n",
        "                     return False\n",
        "        except Exception as e:\n",
        "            block_reason = \"\"\n",
        "            # Try to access potential block reason safely\n",
        "            try:\n",
        "                 # Check if response exists and has the necessary attributes before accessing them\n",
        "                 if response and hasattr(response, 'prompt_feedback') and response.prompt_feedback and hasattr(response.prompt_feedback, 'block_reason') and response.prompt_feedback.block_reason:\n",
        "                      block_reason = f\" (Block Reason: {response.prompt_feedback.block_reason})\"\n",
        "            except AttributeError:\n",
        "                 pass # Ignore if feedback attributes don't exist or response is None\n",
        "\n",
        "            print(f\"      Error calling Gemini API (Attempt {attempt + 1}/{retries + 1}): {e}{block_reason}\")\n",
        "            if attempt < retries:\n",
        "                print(f\"      Retrying in {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(\"      Max retries reached. Treating as NO match.\")\n",
        "                return False\n",
        "    return False\n",
        "\n",
        "\n",
        "# --- Function to calculate metrics AND find matching ranks using LLM ---\n",
        "def calculate_metrics_and_ranks_llm(predictions_data, ground_truth_data, model, k=5):\n",
        "    \"\"\"\n",
        "    Calculates MRR, Mean NDCG@k, Top-1 Accuracy using LLM for matching,\n",
        "    AND stores details of the first match found for validation.\n",
        "\n",
        "    Args:\n",
        "        predictions_data (list): List of prediction dicts.\n",
        "        ground_truth_data (list): List of ground truth dicts.\n",
        "        model (genai.GenerativeModel): The initialized Gemini model.\n",
        "        k (int): The cutoff for calculating NDCG.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (dict: metrics, dict: individual match details) or (None, None)\n",
        "    \"\"\"\n",
        "    if not predictions_data or not ground_truth_data:\n",
        "        print(\"Error: Input data is missing.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        predictions_dict = {item['case_id']: item['differential_diagnosis'] for item in predictions_data}\n",
        "        ground_truth_dict = {item['case_id']: item['correct_diagnosis'] for item in ground_truth_data}\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Missing expected key '{e}' while structuring data.\")\n",
        "        return None, None\n",
        "    except TypeError as e:\n",
        "        print(f\"Error: Problem accessing data, likely incorrect JSON structure: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    reciprocal_ranks = []\n",
        "    ndcg_scores = []\n",
        "    top1_correct_count = 0\n",
        "    processed_cases = 0\n",
        "    cases_with_match = 0\n",
        "    individual_match_details = {} # To store rank and text for validation\n",
        "\n",
        "    common_case_ids = sorted(list(set(predictions_dict.keys()) & set(ground_truth_dict.keys())))\n",
        "\n",
        "    if not common_case_ids:\n",
        "        print(\"Error: No common case_ids found.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\nProcessing {len(common_case_ids)} common cases using LLM ({model.model_name})...\")\n",
        "    print(\"This will take time...\\n\")\n",
        "\n",
        "    for case_id in common_case_ids:\n",
        "        print(f\"--- Processing Case: {case_id} ---\")\n",
        "        if case_id not in predictions_dict or case_id not in ground_truth_dict:\n",
        "            print(\"  Skipped - Data Missing in one of the files.\")\n",
        "            individual_match_details[case_id] = {\"status\": \"Skipped - Data Missing\"}\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "\n",
        "        correct_diagnosis_text = ground_truth_dict[case_id]\n",
        "        predicted_diagnoses_list = predictions_dict[case_id]\n",
        "\n",
        "        if not correct_diagnosis_text:\n",
        "            print(\"  Skipped - No Ground Truth diagnosis text.\")\n",
        "            individual_match_details[case_id] = {\"status\": \"Skipped - No Ground Truth\"}\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "        if not predicted_diagnoses_list:\n",
        "            print(\"  No Match Possible (Prediction list is empty). Assigning zero scores.\")\n",
        "            reciprocal_ranks.append(0)\n",
        "            ndcg_scores.append(0)\n",
        "            individual_match_details[case_id] = {\"status\": \"No Match Found (Empty Predictions)\"}\n",
        "            processed_cases += 1\n",
        "            print(\"-\" * 30)\n",
        "            continue\n",
        "\n",
        "        # Ensure predictions are sorted by rank\n",
        "        predicted_diagnoses_list.sort(key=lambda x: x.get('rank', float('inf')))\n",
        "\n",
        "        # --- LLM Comparison Loop ---\n",
        "        found_rank = 0\n",
        "        first_match_details = {}\n",
        "        for i, prediction_item in enumerate(predicted_diagnoses_list):\n",
        "            current_rank = i + 1\n",
        "            predicted_text = prediction_item.get('diagnosis', '')\n",
        "            print(f\"  Rank {current_rank}: Comparing...\") # Keep output concise\n",
        "\n",
        "            # Call LLM to check match\n",
        "            is_match = check_diagnosis_match_with_gemini(correct_diagnosis_text, predicted_text, model)\n",
        "            time.sleep(1.1) # IMPORTANT: Rate limiting\n",
        "\n",
        "            if is_match:\n",
        "                print(f\"    --> Match found by LLM at rank {current_rank}!\")\n",
        "                found_rank = current_rank\n",
        "                first_match_details = {\n",
        "                    \"rank\": found_rank,\n",
        "                    \"ground_truth\": correct_diagnosis_text,\n",
        "                    \"prediction\": predicted_text\n",
        "                }\n",
        "                break # Stop at the first match\n",
        "\n",
        "        # --- Store results for this case ---\n",
        "        if found_rank > 0:\n",
        "             individual_match_details[case_id] = first_match_details\n",
        "             cases_with_match += 1\n",
        "        else:\n",
        "             individual_match_details[case_id] = {\"status\": \"No Match Found\"}\n",
        "             print(\"  No Match Found for this case.\")\n",
        "\n",
        "\n",
        "        # --- Calculate Metrics based on found_rank ---\n",
        "        rr = 1 / found_rank if found_rank > 0 else 0\n",
        "        reciprocal_ranks.append(rr)\n",
        "\n",
        "        dcg = 0.0\n",
        "        for i in range(min(k, len(predicted_diagnoses_list))):\n",
        "            rank_in_list = i + 1\n",
        "            relevance = 1 if rank_in_list == found_rank else 0\n",
        "            dcg += relevance / math.log2(rank_in_list + 1)\n",
        "\n",
        "        idcg = 1.0 / math.log2(1 + 1) if found_rank > 0 else 0.0\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "        ndcg_scores.append(ndcg)\n",
        "\n",
        "        if found_rank == 1:\n",
        "            top1_correct_count += 1\n",
        "\n",
        "        processed_cases += 1\n",
        "        print(\"-\" * 30) # Separator between cases\n",
        "\n",
        "    # --- Aggregate Results ---\n",
        "    if processed_cases == 0:\n",
        "        print(\"Error: No cases were successfully processed.\")\n",
        "        return None, None\n",
        "\n",
        "    mean_mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "    mean_ndcg_at_k = np.mean(ndcg_scores) if ndcg_scores else 0\n",
        "    top_1_accuracy = top1_correct_count / processed_cases if processed_cases > 0 else 0\n",
        "\n",
        "    metrics = {\n",
        "        \"total_cases_processed\": processed_cases,\n",
        "        \"cases_with_llm_match_found\": cases_with_match,\n",
        "        \"llm_model_used\": model.model_name,\n",
        "        \"mrr\": mean_mrr,\n",
        "        f\"mean_ndcg@{k}\": mean_ndcg_at_k,\n",
        "        \"top_1_accuracy\": top_1_accuracy,\n",
        "        \"k_for_ndcg\": k\n",
        "    }\n",
        "    return metrics, individual_match_details\n",
        "\n",
        "# --- Configuration ---\n",
        "LLM_MODEL_NAME = 'gemini-2.5-pro-preview-03-25' # Your specified model\n",
        "NDCG_K = 5 # Rank cutoff for NDCG calculation\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Mount Drive ---\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        DRIVE_ROOT = \"/content/drive/MyDrive/\"\n",
        "        print(\"Google Drive mounted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Configure Gemini API ---\n",
        "    try:\n",
        "        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "        if not GOOGLE_API_KEY:\n",
        "            raise ValueError(\"API Key not found in Colab Secrets\")\n",
        "        genai.configure(api_key=GOOGLE_API_KEY)\n",
        "        print(\"Gemini API configured.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring Gemini API: {e}\")\n",
        "        print(\"Please ensure you have set the 'GOOGLE_API_KEY' secret in Colab.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Initialize the Gemini Model ---\n",
        "    try:\n",
        "        print(f\"Initializing Gemini model: {LLM_MODEL_NAME}...\")\n",
        "        model = genai.GenerativeModel(LLM_MODEL_NAME)\n",
        "        print(\"Model initialized.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Gemini model: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # --- Get File Paths ---\n",
        "    print(\"\\n--- Specify Input JSON File Paths ---\")\n",
        "    print(\"Use the Colab file browser (left panel) to copy the *full path* for each file.\")\n",
        "    ground_truth_file_path = input(\"1. Paste the full path to 'ground_truth_diagnoses.json': \").strip()\n",
        "    predictions_file_path = input(\"2. Paste the full path to 'perplexity_research_predictions.json': \").strip() # Use the ranked one\n",
        "\n",
        "    if not ground_truth_file_path or not predictions_file_path:\n",
        "        print(\"Error: One or both file paths were not provided. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Load Data ---\n",
        "    print(\"\\nLoading data...\")\n",
        "    ground_truth_data = load_json_data(ground_truth_file_path)\n",
        "    predictions_data = load_json_data(predictions_file_path)\n",
        "\n",
        "    if ground_truth_data is None or predictions_data is None:\n",
        "        print(\"Failed to load data. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Calculate Metrics and Find Ranks using LLM ---\n",
        "    metrics_results, individual_match_info = calculate_metrics_and_ranks_llm(\n",
        "        predictions_data,\n",
        "        ground_truth_data,\n",
        "        model,\n",
        "        k=NDCG_K\n",
        "    )\n",
        "\n",
        "    # --- Display Individual Match Details (for Validation) ---\n",
        "    if individual_match_info:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"--- Individual Case Match Details (Rank of First LLM Match) ---\")\n",
        "        print(\"=\"*60)\n",
        "        for case_id, details in individual_match_info.items():\n",
        "            print(f\"{case_id}:\")\n",
        "            if \"rank\" in details: # Check if a match was found\n",
        "                print(f\"  Rank {details['rank']}: '{details['ground_truth']}' vs '{details['prediction']}'\")\n",
        "            else:\n",
        "                print(f\"  {details.get('status', 'Unknown Status')}\") # Print status like 'No Match Found' or 'Skipped'\n",
        "            print(\"-\" * 30)\n",
        "    else:\n",
        "        print(\"\\nCould not retrieve individual match details.\")\n",
        "\n",
        "\n",
        "    # --- Display Overall Metrics ---\n",
        "    if metrics_results:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"--- Overall Evaluation Results (LLM-based) ---\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"LLM Model Used: {metrics_results['llm_model_used']}\")\n",
        "        print(f\"Total Cases Processed: {metrics_results['total_cases_processed']}\")\n",
        "        print(f\"Cases Where LLM Found a Match: {metrics_results['cases_with_llm_match_found']} ({metrics_results['cases_with_llm_match_found']/metrics_results['total_cases_processed']:.1%})\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"MRR (Mean Reciprocal Rank): {metrics_results['mrr']:.4f}\")\n",
        "\n",
        "        k_value = metrics_results['k_for_ndcg']\n",
        "        ndcg_key = f\"mean_ndcg@{k_value}\"\n",
        "        if ndcg_key in metrics_results:\n",
        "             print(f\"Mean NDCG@{k_value}: {metrics_results[ndcg_key]:.4f}\")\n",
        "        else:\n",
        "             print(f\"Mean NDCG@{k_value}: Key '{ndcg_key}' not found in results.\")\n",
        "\n",
        "        print(f\"Top-1 Accuracy (LLM match at rank 1): {metrics_results['top_1_accuracy']:.2%}\")\n",
        "        print(\"=\"*60)\n",
        "    else:\n",
        "        print(\"\\nFailed to calculate overall metrics.\")\n",
        "\n",
        "    print(\"\\nScript finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "ydYRmpMFIwUh",
        "outputId": "aa87c767-78b8-4bea-bc6b-9d304af4c009"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n",
            "Gemini API configured.\n",
            "Initializing Gemini model: gemini-2.5-pro-preview-03-25...\n",
            "Model initialized.\n",
            "\n",
            "--- Specify Input JSON File Paths ---\n",
            "Use the Colab file browser (left panel) to copy the *full path* for each file.\n",
            "1. Paste the full path to 'ground_truth_diagnoses.json': /content/drive/MyDrive/BADM550 - Wolters Kluwer Health - Language Model Project/Full Case Records/ground_truth_diagnoses.json\n",
            "2. Paste the full path to 'perplexity_research_predictions.json': /content/drive/MyDrive/chatGPT_o4-mini-high_predictions.json\n",
            "\n",
            "Loading data...\n",
            "\n",
            "Processing 1 common cases using LLM (models/gemini-2.5-pro-preview-03-25)...\n",
            "This will take time...\n",
            "\n",
            "--- Processing Case: NEJMcpc2100279 ---\n",
            "  Rank 1: Comparing...\n",
            "  Rank 2: Comparing...\n",
            "  Rank 3: Comparing...\n",
            "  Rank 4: Comparing...\n",
            "  Rank 5: Comparing...\n",
            "  No Match Found for this case.\n",
            "------------------------------\n",
            "\n",
            "============================================================\n",
            "--- Individual Case Match Details (Rank of First LLM Match) ---\n",
            "============================================================\n",
            "NEJMcpc2100279:\n",
            "  No Match Found\n",
            "------------------------------\n",
            "\n",
            "============================================================\n",
            "--- Overall Evaluation Results (LLM-based) ---\n",
            "============================================================\n",
            "LLM Model Used: models/gemini-2.5-pro-preview-03-25\n",
            "Total Cases Processed: 1\n",
            "Cases Where LLM Found a Match: 0 (0.0%)\n",
            "-------------------------\n",
            "MRR (Mean Reciprocal Rank): 0.0000\n",
            "Mean NDCG@5: 0.0000\n",
            "Top-1 Accuracy (LLM match at rank 1): 0.00%\n",
            "============================================================\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ]
    }
  ]
}